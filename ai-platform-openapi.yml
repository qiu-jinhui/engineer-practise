openapi: 3.0.0
info:
  title: AI Platform API
  description: The AI Platform REST API. 
  version: 1.0.0
  contact:
    name: AI Platform Support Team
servers:
  - url: https://api.aiplatform.hsbc/v1
security:
  - ApiKeyAuth: []
tags:
  - name: Chat
    description: Given a list of messages comprising a conversation, the model will return a response.
  - name: Completions
    description: >-
      Given a prompt, the model will return one or more predicted completions, and can also return the
      probabilities of alternative tokens at each position.
  - name: Embeddings
    description: >-
      Get a vector representation of a given input that can be easily consumed by machine learning models and
      algorithms.
  - name: Models
    description: List and describe the various models available in the API.
  - name: Moderations
    description: Given text and/or image inputs, classifies if those inputs are potentially harmful.
paths:
  /chat/completions:
    post:
      operationId: createChatCompletion
      tags:
        - Chat
      summary: |
        Creates a model response for the given chat conversation. 

        Parameter support can differ depending on the model used to generate the
        response, particularly for newer reasoning models. Parameters that are only
        supported for reasoning models are noted below. For the current state of 
        unsupported parameters in reasoning models, 
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/CreateChatCompletionRequest'
      responses:
        '200':
          description: OK
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/CreateChatCompletionResponse'
            text/event-stream:
              schema:
                $ref: '#/components/schemas/CreateChatCompletionStreamResponse'
      x-oaiMeta:
        name: Create chat completion
        group: chat
        returns: >
          Returns a [chat completion](/docs/api-reference/chat/object) object, or a streamed sequence of [chat
          completion chunk](/docs/api-reference/chat/streaming) objects if the request is streamed.
        path: create
        examples:
          - title: Default
            request:
              curl: |
                curl https://api.openai.com/v1/chat/completions \
                  -H "Content-Type: application/json" \
                  -H "Authorization: Bearer $OPENAI_API_KEY" \
                  -d '{
                    "model": "VAR_chat_model_id",
                    "messages": [
                      {
                        "role": "developer",
                        "content": "You are a helpful assistant."
                      },
                      {
                        "role": "user",
                        "content": "Hello!"
                      }
                    ]
                  }'
              python: |-
                import os
                from openai import OpenAI

                client = OpenAI(
                    api_key=os.environ.get("OPENAI_API_KEY"),  # This is the default and can be omitted
                )
                chat_completion = client.chat.completions.create(
                    messages=[{
                        "content": "string",
                        "role": "developer",
                    }],
                    model="gpt-4o",
                )
                print(chat_completion)
              node.js: |-
                import OpenAI from 'openai';

                const client = new OpenAI({
                  apiKey: process.env['OPENAI_API_KEY'], // This is the default and can be omitted
                });

                const chatCompletion = await client.chat.completions.create({
                  messages: [{ content: 'string', role: 'developer' }],
                  model: 'gpt-4o',
                });

                console.log(chatCompletion);
              csharp: |
                using System;
                using System.Collections.Generic;

                using OpenAI.Chat;

                ChatClient client = new(
                    model: "gpt-4.1",
                    apiKey: Environment.GetEnvironmentVariable("OPENAI_API_KEY")
                );

                List<ChatMessage> messages =
                [
                    new SystemChatMessage("You are a helpful assistant."),
                    new UserChatMessage("Hello!")
                ];

                ChatCompletion completion = client.CompleteChat(messages);

                Console.WriteLine(completion.Content[0].Text);
              go: |
                package main

                import (
                  "context"
                  "fmt"

                  "github.com/openai/openai-go"
                  "github.com/openai/openai-go/option"
                  "github.com/openai/openai-go/shared"
                )

                func main() {
                  client := openai.NewClient(
                    option.WithAPIKey("My API Key"), // defaults to os.LookupEnv("OPENAI_API_KEY")
                  )
                  chatCompletion, err := client.Chat.Completions.New(context.TODO(), openai.ChatCompletionNewParams{
                    Messages: []openai.ChatCompletionMessageParamUnion{openai.ChatCompletionMessageParamUnion{
                      OfDeveloper: &openai.ChatCompletionDeveloperMessageParam{
                        Content: openai.ChatCompletionDeveloperMessageParamContentUnion{
                          OfString: openai.String("string"),
                        },
                      },
                    }},
                    Model: shared.ChatModelGPT4_1,
                  })
                  if err != nil {
                    panic(err.Error())
                  }
                  fmt.Printf("%+v\n", chatCompletion)
                }
              java: |-
                package com.openai.example;

                import com.openai.client.OpenAIClient;
                import com.openai.client.okhttp.OpenAIOkHttpClient;
                import com.openai.models.ChatModel;
                import com.openai.models.chat.completions.ChatCompletion;
                import com.openai.models.chat.completions.ChatCompletionCreateParams;

                public final class Main {
                    private Main() {}

                    public static void main(String[] args) {
                        // Configures using the `OPENAI_API_KEY`, `OPENAI_ORG_ID`, `OPENAI_PROJECT_ID`, `OPENAI_WEBHOOK_SECRET` and `OPENAI_BASE_URL` environment variables
                        OpenAIClient client = OpenAIOkHttpClient.fromEnv();

                        ChatCompletionCreateParams params = ChatCompletionCreateParams.builder()
                            .addDeveloperMessage("string")
                            .model(ChatModel.GPT_4_1)
                            .build();
                        ChatCompletion chatCompletion = client.chat().completions().create(params);
                    }
                }
              kotlin: |-
                package com.openai.example

                import com.openai.client.OpenAIClient
                import com.openai.client.okhttp.OpenAIOkHttpClient
                import com.openai.models.ChatModel
                import com.openai.models.chat.completions.ChatCompletion
                import com.openai.models.chat.completions.ChatCompletionCreateParams

                fun main() {
                    // Configures using the `OPENAI_API_KEY`, `OPENAI_ORG_ID`, `OPENAI_PROJECT_ID`, `OPENAI_WEBHOOK_SECRET` and `OPENAI_BASE_URL` environment variables
                    val client: OpenAIClient = OpenAIOkHttpClient.fromEnv()

                    val params: ChatCompletionCreateParams = ChatCompletionCreateParams.builder()
                        .addDeveloperMessage("string")
                        .model(ChatModel.GPT_4_1)
                        .build()
                    val chatCompletion: ChatCompletion = client.chat().completions().create(params)
                }
              ruby: >-
                require "openai"


                openai = OpenAI::Client.new(
                  api_key: ENV["OPENAI_API_KEY"] # This is the default and can be omitted
                )


                chat_completion = openai.chat.completions.create(messages: [{content: "string", role:
                :developer}], model: :"gpt-4.1")


                puts(chat_completion)
            response: |
              {
                "id": "chatcmpl-B9MBs8CjcvOU2jLn4n570S5qMJKcT",
                "object": "chat.completion",
                "created": 1741569952,
                "model": "gpt-4.1-2025-04-14",
                "choices": [
                  {
                    "index": 0,
                    "message": {
                      "role": "assistant",
                      "content": "Hello! How can I assist you today?",
                      "refusal": null,
                      "annotations": []
                    },
                    "logprobs": null,
                    "finish_reason": "stop"
                  }
                ],
                "usage": {
                  "prompt_tokens": 19,
                  "completion_tokens": 10,
                  "total_tokens": 29,
                  "prompt_tokens_details": {
                    "cached_tokens": 0,
                    "audio_tokens": 0
                  },
                  "completion_tokens_details": {
                    "reasoning_tokens": 0,
                    "audio_tokens": 0,
                    "accepted_prediction_tokens": 0,
                    "rejected_prediction_tokens": 0
                  }
                },
                "service_tier": "default"
              }
          - title: Image input
            request:
              curl: |
                curl https://api.openai.com/v1/chat/completions \
                  -H "Content-Type: application/json" \
                  -H "Authorization: Bearer $OPENAI_API_KEY" \
                  -d '{
                    "model": "gpt-4.1",
                    "messages": [
                      {
                        "role": "user",
                        "content": [
                          {
                            "type": "text",
                            "text": "What is in this image?"
                          },
                          {
                            "type": "image_url",
                            "image_url": {
                              "url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg"
                            }
                          }
                        ]
                      }
                    ],
                    "max_tokens": 300
                  }'
              python: |-
                import os
                from openai import OpenAI

                client = OpenAI(
                    api_key=os.environ.get("OPENAI_API_KEY"),  # This is the default and can be omitted
                )
                chat_completion = client.chat.completions.create(
                    messages=[{
                        "content": "string",
                        "role": "developer",
                    }],
                    model="gpt-4o",
                )
                print(chat_completion)
              node.js: |-
                import OpenAI from 'openai';

                const client = new OpenAI({
                  apiKey: process.env['OPENAI_API_KEY'], // This is the default and can be omitted
                });

                const chatCompletion = await client.chat.completions.create({
                  messages: [{ content: 'string', role: 'developer' }],
                  model: 'gpt-4o',
                });

                console.log(chatCompletion);
              csharp: |
                using System;
                using System.Collections.Generic;

                using OpenAI.Chat;

                ChatClient client = new(
                    model: "gpt-4.1",
                    apiKey: Environment.GetEnvironmentVariable("OPENAI_API_KEY")
                );

                List<ChatMessage> messages =
                [
                    new UserChatMessage(
                    [
                        ChatMessageContentPart.CreateTextPart("What's in this image?"),
                        ChatMessageContentPart.CreateImagePart(new Uri("https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg"))
                    ])
                ];

                ChatCompletion completion = client.CompleteChat(messages);

                Console.WriteLine(completion.Content[0].Text);
              go: |
                package main

                import (
                  "context"
                  "fmt"

                  "github.com/openai/openai-go"
                  "github.com/openai/openai-go/option"
                  "github.com/openai/openai-go/shared"
                )

                func main() {
                  client := openai.NewClient(
                    option.WithAPIKey("My API Key"), // defaults to os.LookupEnv("OPENAI_API_KEY")
                  )
                  chatCompletion, err := client.Chat.Completions.New(context.TODO(), openai.ChatCompletionNewParams{
                    Messages: []openai.ChatCompletionMessageParamUnion{openai.ChatCompletionMessageParamUnion{
                      OfDeveloper: &openai.ChatCompletionDeveloperMessageParam{
                        Content: openai.ChatCompletionDeveloperMessageParamContentUnion{
                          OfString: openai.String("string"),
                        },
                      },
                    }},
                    Model: shared.ChatModelGPT4_1,
                  })
                  if err != nil {
                    panic(err.Error())
                  }
                  fmt.Printf("%+v\n", chatCompletion)
                }
              java: |-
                package com.openai.example;

                import com.openai.client.OpenAIClient;
                import com.openai.client.okhttp.OpenAIOkHttpClient;
                import com.openai.models.ChatModel;
                import com.openai.models.chat.completions.ChatCompletion;
                import com.openai.models.chat.completions.ChatCompletionCreateParams;

                public final class Main {
                    private Main() {}

                    public static void main(String[] args) {
                        // Configures using the `OPENAI_API_KEY`, `OPENAI_ORG_ID`, `OPENAI_PROJECT_ID`, `OPENAI_WEBHOOK_SECRET` and `OPENAI_BASE_URL` environment variables
                        OpenAIClient client = OpenAIOkHttpClient.fromEnv();

                        ChatCompletionCreateParams params = ChatCompletionCreateParams.builder()
                            .addDeveloperMessage("string")
                            .model(ChatModel.GPT_4_1)
                            .build();
                        ChatCompletion chatCompletion = client.chat().completions().create(params);
                    }
                }
              kotlin: |-
                package com.openai.example

                import com.openai.client.OpenAIClient
                import com.openai.client.okhttp.OpenAIOkHttpClient
                import com.openai.models.ChatModel
                import com.openai.models.chat.completions.ChatCompletion
                import com.openai.models.chat.completions.ChatCompletionCreateParams

                fun main() {
                    // Configures using the `OPENAI_API_KEY`, `OPENAI_ORG_ID`, `OPENAI_PROJECT_ID`, `OPENAI_WEBHOOK_SECRET` and `OPENAI_BASE_URL` environment variables
                    val client: OpenAIClient = OpenAIOkHttpClient.fromEnv()

                    val params: ChatCompletionCreateParams = ChatCompletionCreateParams.builder()
                        .addDeveloperMessage("string")
                        .model(ChatModel.GPT_4_1)
                        .build()
                    val chatCompletion: ChatCompletion = client.chat().completions().create(params)
                }
              ruby: >-
                require "openai"


                openai = OpenAI::Client.new(
                  api_key: ENV["OPENAI_API_KEY"] # This is the default and can be omitted
                )


                chat_completion = openai.chat.completions.create(messages: [{content: "string", role:
                :developer}], model: :"gpt-4.1")


                puts(chat_completion)
            response: |
              {
                "id": "chatcmpl-B9MHDbslfkBeAs8l4bebGdFOJ6PeG",
                "object": "chat.completion",
                "created": 1741570283,
                "model": "gpt-4.1-2025-04-14",
                "choices": [
                  {
                    "index": 0,
                    "message": {
                      "role": "assistant",
                      "content": "The image shows a wooden boardwalk path running through a lush green field or meadow. The sky is bright blue with some scattered clouds, giving the scene a serene and peaceful atmosphere. Trees and shrubs are visible in the background.",
                      "refusal": null,
                      "annotations": []
                    },
                    "logprobs": null,
                    "finish_reason": "stop"
                  }
                ],
                "usage": {
                  "prompt_tokens": 1117,
                  "completion_tokens": 46,
                  "total_tokens": 1163,
                  "prompt_tokens_details": {
                    "cached_tokens": 0,
                    "audio_tokens": 0
                  },
                  "completion_tokens_details": {
                    "reasoning_tokens": 0,
                    "audio_tokens": 0,
                    "accepted_prediction_tokens": 0,
                    "rejected_prediction_tokens": 0
                  }
                },
                "service_tier": "default"
              }
          - title: Streaming
            request:
              curl: |
                curl https://api.openai.com/v1/chat/completions \
                  -H "Content-Type: application/json" \
                  -H "Authorization: Bearer $OPENAI_API_KEY" \
                  -d '{
                    "model": "VAR_chat_model_id",
                    "messages": [
                      {
                        "role": "developer",
                        "content": "You are a helpful assistant."
                      },
                      {
                        "role": "user",
                        "content": "Hello!"
                      }
                    ],
                    "stream": true
                  }'
              python: |-
                import os
                from openai import OpenAI

                client = OpenAI(
                    api_key=os.environ.get("OPENAI_API_KEY"),  # This is the default and can be omitted
                )
                chat_completion = client.chat.completions.create(
                    messages=[{
                        "content": "string",
                        "role": "developer",
                    }],
                    model="gpt-4o",
                )
                print(chat_completion)
              node.js: |-
                import OpenAI from 'openai';

                const client = new OpenAI({
                  apiKey: process.env['OPENAI_API_KEY'], // This is the default and can be omitted
                });

                const chatCompletion = await client.chat.completions.create({
                  messages: [{ content: 'string', role: 'developer' }],
                  model: 'gpt-4o',
                });

                console.log(chatCompletion);
              csharp: >
                using System;

                using System.ClientModel;

                using System.Collections.Generic;

                using System.Threading.Tasks;


                using OpenAI.Chat;


                ChatClient client = new(
                    model: "gpt-4.1",
                    apiKey: Environment.GetEnvironmentVariable("OPENAI_API_KEY")
                );


                List<ChatMessage> messages =

                [
                    new SystemChatMessage("You are a helpful assistant."),
                    new UserChatMessage("Hello!")
                ];


                AsyncCollectionResult<StreamingChatCompletionUpdate> completionUpdates =
                client.CompleteChatStreamingAsync(messages);


                await foreach (StreamingChatCompletionUpdate completionUpdate in completionUpdates)

                {
                    if (completionUpdate.ContentUpdate.Count > 0)
                    {
                        Console.Write(completionUpdate.ContentUpdate[0].Text);
                    }
                }
              go: |
                package main

                import (
                  "context"
                  "fmt"

                  "github.com/openai/openai-go"
                  "github.com/openai/openai-go/option"
                  "github.com/openai/openai-go/shared"
                )

                func main() {
                  client := openai.NewClient(
                    option.WithAPIKey("My API Key"), // defaults to os.LookupEnv("OPENAI_API_KEY")
                  )
                  chatCompletion, err := client.Chat.Completions.New(context.TODO(), openai.ChatCompletionNewParams{
                    Messages: []openai.ChatCompletionMessageParamUnion{openai.ChatCompletionMessageParamUnion{
                      OfDeveloper: &openai.ChatCompletionDeveloperMessageParam{
                        Content: openai.ChatCompletionDeveloperMessageParamContentUnion{
                          OfString: openai.String("string"),
                        },
                      },
                    }},
                    Model: shared.ChatModelGPT4_1,
                  })
                  if err != nil {
                    panic(err.Error())
                  }
                  fmt.Printf("%+v\n", chatCompletion)
                }
              java: |-
                package com.openai.example;

                import com.openai.client.OpenAIClient;
                import com.openai.client.okhttp.OpenAIOkHttpClient;
                import com.openai.models.ChatModel;
                import com.openai.models.chat.completions.ChatCompletion;
                import com.openai.models.chat.completions.ChatCompletionCreateParams;

                public final class Main {
                    private Main() {}

                    public static void main(String[] args) {
                        // Configures using the `OPENAI_API_KEY`, `OPENAI_ORG_ID`, `OPENAI_PROJECT_ID`, `OPENAI_WEBHOOK_SECRET` and `OPENAI_BASE_URL` environment variables
                        OpenAIClient client = OpenAIOkHttpClient.fromEnv();

                        ChatCompletionCreateParams params = ChatCompletionCreateParams.builder()
                            .addDeveloperMessage("string")
                            .model(ChatModel.GPT_4_1)
                            .build();
                        ChatCompletion chatCompletion = client.chat().completions().create(params);
                    }
                }
              kotlin: |-
                package com.openai.example

                import com.openai.client.OpenAIClient
                import com.openai.client.okhttp.OpenAIOkHttpClient
                import com.openai.models.ChatModel
                import com.openai.models.chat.completions.ChatCompletion
                import com.openai.models.chat.completions.ChatCompletionCreateParams

                fun main() {
                    // Configures using the `OPENAI_API_KEY`, `OPENAI_ORG_ID`, `OPENAI_PROJECT_ID`, `OPENAI_WEBHOOK_SECRET` and `OPENAI_BASE_URL` environment variables
                    val client: OpenAIClient = OpenAIOkHttpClient.fromEnv()

                    val params: ChatCompletionCreateParams = ChatCompletionCreateParams.builder()
                        .addDeveloperMessage("string")
                        .model(ChatModel.GPT_4_1)
                        .build()
                    val chatCompletion: ChatCompletion = client.chat().completions().create(params)
                }
              ruby: >-
                require "openai"


                openai = OpenAI::Client.new(
                  api_key: ENV["OPENAI_API_KEY"] # This is the default and can be omitted
                )


                chat_completion = openai.chat.completions.create(messages: [{content: "string", role:
                :developer}], model: :"gpt-4.1")


                puts(chat_completion)
            response: >
              {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1694268190,"model":"gpt-4o-mini",
              "system_fingerprint": "fp_44709d6fcb",
              "choices":[{"index":0,"delta":{"role":"assistant","content":""},"logprobs":null,"finish_reason":null}]}


              {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1694268190,"model":"gpt-4o-mini",
              "system_fingerprint": "fp_44709d6fcb",
              "choices":[{"index":0,"delta":{"content":"Hello"},"logprobs":null,"finish_reason":null}]}


              ....


              {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1694268190,"model":"gpt-4o-mini",
              "system_fingerprint": "fp_44709d6fcb",
              "choices":[{"index":0,"delta":{},"logprobs":null,"finish_reason":"stop"}]}
          - title: Functions
            request:
              curl: |
                curl https://api.openai.com/v1/chat/completions \
                -H "Content-Type: application/json" \
                -H "Authorization: Bearer $OPENAI_API_KEY" \
                -d '{
                  "model": "gpt-4.1",
                  "messages": [
                    {
                      "role": "user",
                      "content": "What is the weather like in Boston today?"
                    }
                  ],
                  "tools": [
                    {
                      "type": "function",
                      "function": {
                        "name": "get_current_weather",
                        "description": "Get the current weather in a given location",
                        "parameters": {
                          "type": "object",
                          "properties": {
                            "location": {
                              "type": "string",
                              "description": "The city and state, e.g. San Francisco, CA"
                            },
                            "unit": {
                              "type": "string",
                              "enum": ["celsius", "fahrenheit"]
                            }
                          },
                          "required": ["location"]
                        }
                      }
                    }
                  ],
                  "tool_choice": "auto"
                }'
              python: |-
                import os
                from openai import OpenAI

                client = OpenAI(
                    api_key=os.environ.get("OPENAI_API_KEY"),  # This is the default and can be omitted
                )
                chat_completion = client.chat.completions.create(
                    messages=[{
                        "content": "string",
                        "role": "developer",
                    }],
                    model="gpt-4o",
                )
                print(chat_completion)
              node.js: |-
                import OpenAI from 'openai';

                const client = new OpenAI({
                  apiKey: process.env['OPENAI_API_KEY'], // This is the default and can be omitted
                });

                const chatCompletion = await client.chat.completions.create({
                  messages: [{ content: 'string', role: 'developer' }],
                  model: 'gpt-4o',
                });

                console.log(chatCompletion);
              csharp: |
                using System;
                using System.Collections.Generic;

                using OpenAI.Chat;

                ChatClient client = new(
                    model: "gpt-4.1",
                    apiKey: Environment.GetEnvironmentVariable("OPENAI_API_KEY")
                );

                ChatTool getCurrentWeatherTool = ChatTool.CreateFunctionTool(
                    functionName: "get_current_weather",
                    functionDescription: "Get the current weather in a given location",
                    functionParameters: BinaryData.FromString("""
                        {
                            "type": "object",
                            "properties": {
                                "location": {
                                    "type": "string",
                                    "description": "The city and state, e.g. San Francisco, CA"
                                },
                                "unit": {
                                    "type": "string",
                                    "enum": [ "celsius", "fahrenheit" ]
                                }
                            },
                            "required": [ "location" ]
                        }
                    """)
                );

                List<ChatMessage> messages =
                [
                    new UserChatMessage("What's the weather like in Boston today?"),
                ];

                ChatCompletionOptions options = new()
                {
                    Tools =
                    {
                        getCurrentWeatherTool
                    },
                    ToolChoice = ChatToolChoice.CreateAutoChoice(),
                };

                ChatCompletion completion = client.CompleteChat(messages, options);
              go: |
                package main

                import (
                  "context"
                  "fmt"

                  "github.com/openai/openai-go"
                  "github.com/openai/openai-go/option"
                  "github.com/openai/openai-go/shared"
                )

                func main() {
                  client := openai.NewClient(
                    option.WithAPIKey("My API Key"), // defaults to os.LookupEnv("OPENAI_API_KEY")
                  )
                  chatCompletion, err := client.Chat.Completions.New(context.TODO(), openai.ChatCompletionNewParams{
                    Messages: []openai.ChatCompletionMessageParamUnion{openai.ChatCompletionMessageParamUnion{
                      OfDeveloper: &openai.ChatCompletionDeveloperMessageParam{
                        Content: openai.ChatCompletionDeveloperMessageParamContentUnion{
                          OfString: openai.String("string"),
                        },
                      },
                    }},
                    Model: shared.ChatModelGPT4_1,
                  })
                  if err != nil {
                    panic(err.Error())
                  }
                  fmt.Printf("%+v\n", chatCompletion)
                }
              java: |-
                package com.openai.example;

                import com.openai.client.OpenAIClient;
                import com.openai.client.okhttp.OpenAIOkHttpClient;
                import com.openai.models.ChatModel;
                import com.openai.models.chat.completions.ChatCompletion;
                import com.openai.models.chat.completions.ChatCompletionCreateParams;

                public final class Main {
                    private Main() {}

                    public static void main(String[] args) {
                        // Configures using the `OPENAI_API_KEY`, `OPENAI_ORG_ID`, `OPENAI_PROJECT_ID`, `OPENAI_WEBHOOK_SECRET` and `OPENAI_BASE_URL` environment variables
                        OpenAIClient client = OpenAIOkHttpClient.fromEnv();

                        ChatCompletionCreateParams params = ChatCompletionCreateParams.builder()
                            .addDeveloperMessage("string")
                            .model(ChatModel.GPT_4_1)
                            .build();
                        ChatCompletion chatCompletion = client.chat().completions().create(params);
                    }
                }
              kotlin: |-
                package com.openai.example

                import com.openai.client.OpenAIClient
                import com.openai.client.okhttp.OpenAIOkHttpClient
                import com.openai.models.ChatModel
                import com.openai.models.chat.completions.ChatCompletion
                import com.openai.models.chat.completions.ChatCompletionCreateParams

                fun main() {
                    // Configures using the `OPENAI_API_KEY`, `OPENAI_ORG_ID`, `OPENAI_PROJECT_ID`, `OPENAI_WEBHOOK_SECRET` and `OPENAI_BASE_URL` environment variables
                    val client: OpenAIClient = OpenAIOkHttpClient.fromEnv()

                    val params: ChatCompletionCreateParams = ChatCompletionCreateParams.builder()
                        .addDeveloperMessage("string")
                        .model(ChatModel.GPT_4_1)
                        .build()
                    val chatCompletion: ChatCompletion = client.chat().completions().create(params)
                }
              ruby: >-
                require "openai"


                openai = OpenAI::Client.new(
                  api_key: ENV["OPENAI_API_KEY"] # This is the default and can be omitted
                )


                chat_completion = openai.chat.completions.create(messages: [{content: "string", role:
                :developer}], model: :"gpt-4.1")


                puts(chat_completion)
            response: |
              {
                "id": "chatcmpl-abc123",
                "object": "chat.completion",
                "created": 1699896916,
                "model": "gpt-4o-mini",
                "choices": [
                  {
                    "index": 0,
                    "message": {
                      "role": "assistant",
                      "content": null,
                      "tool_calls": [
                        {
                          "id": "call_abc123",
                          "type": "function",
                          "function": {
                            "name": "get_current_weather",
                            "arguments": "{\n\"location\": \"Boston, MA\"\n}"
                          }
                        }
                      ]
                    },
                    "logprobs": null,
                    "finish_reason": "tool_calls"
                  }
                ],
                "usage": {
                  "prompt_tokens": 82,
                  "completion_tokens": 17,
                  "total_tokens": 99,
                  "completion_tokens_details": {
                    "reasoning_tokens": 0,
                    "accepted_prediction_tokens": 0,
                    "rejected_prediction_tokens": 0
                  }
                }
              }
          - title: Logprobs
            request:
              curl: |
                curl https://api.openai.com/v1/chat/completions \
                  -H "Content-Type: application/json" \
                  -H "Authorization: Bearer $OPENAI_API_KEY" \
                  -d '{
                    "model": "VAR_chat_model_id",
                    "messages": [
                      {
                        "role": "user",
                        "content": "Hello!"
                      }
                    ],
                    "logprobs": true,
                    "top_logprobs": 2
                  }'
              python: |-
                import os
                from openai import OpenAI

                client = OpenAI(
                    api_key=os.environ.get("OPENAI_API_KEY"),  # This is the default and can be omitted
                )
                chat_completion = client.chat.completions.create(
                    messages=[{
                        "content": "string",
                        "role": "developer",
                    }],
                    model="gpt-4o",
                )
                print(chat_completion)
              node.js: |-
                import OpenAI from 'openai';

                const client = new OpenAI({
                  apiKey: process.env['OPENAI_API_KEY'], // This is the default and can be omitted
                });

                const chatCompletion = await client.chat.completions.create({
                  messages: [{ content: 'string', role: 'developer' }],
                  model: 'gpt-4o',
                });

                console.log(chatCompletion);
              csharp: |
                using System;
                using System.Collections.Generic;

                using OpenAI.Chat;

                ChatClient client = new(
                    model: "gpt-4.1",
                    apiKey: Environment.GetEnvironmentVariable("OPENAI_API_KEY")
                );

                List<ChatMessage> messages =
                [
                    new UserChatMessage("Hello!")
                ];

                ChatCompletionOptions options = new()
                {
                    IncludeLogProbabilities = true,
                    TopLogProbabilityCount = 2
                };

                ChatCompletion completion = client.CompleteChat(messages, options);

                Console.WriteLine(completion.Content[0].Text);
              go: |
                package main

                import (
                  "context"
                  "fmt"

                  "github.com/openai/openai-go"
                  "github.com/openai/openai-go/option"
                  "github.com/openai/openai-go/shared"
                )

                func main() {
                  client := openai.NewClient(
                    option.WithAPIKey("My API Key"), // defaults to os.LookupEnv("OPENAI_API_KEY")
                  )
                  chatCompletion, err := client.Chat.Completions.New(context.TODO(), openai.ChatCompletionNewParams{
                    Messages: []openai.ChatCompletionMessageParamUnion{openai.ChatCompletionMessageParamUnion{
                      OfDeveloper: &openai.ChatCompletionDeveloperMessageParam{
                        Content: openai.ChatCompletionDeveloperMessageParamContentUnion{
                          OfString: openai.String("string"),
                        },
                      },
                    }},
                    Model: shared.ChatModelGPT4_1,
                  })
                  if err != nil {
                    panic(err.Error())
                  }
                  fmt.Printf("%+v\n", chatCompletion)
                }
              java: |-
                package com.openai.example;

                import com.openai.client.OpenAIClient;
                import com.openai.client.okhttp.OpenAIOkHttpClient;
                import com.openai.models.ChatModel;
                import com.openai.models.chat.completions.ChatCompletion;
                import com.openai.models.chat.completions.ChatCompletionCreateParams;

                public final class Main {
                    private Main() {}

                    public static void main(String[] args) {
                        // Configures using the `OPENAI_API_KEY`, `OPENAI_ORG_ID`, `OPENAI_PROJECT_ID`, `OPENAI_WEBHOOK_SECRET` and `OPENAI_BASE_URL` environment variables
                        OpenAIClient client = OpenAIOkHttpClient.fromEnv();

                        ChatCompletionCreateParams params = ChatCompletionCreateParams.builder()
                            .addDeveloperMessage("string")
                            .model(ChatModel.GPT_4_1)
                            .build();
                        ChatCompletion chatCompletion = client.chat().completions().create(params);
                    }
                }
              kotlin: |-
                package com.openai.example

                import com.openai.client.OpenAIClient
                import com.openai.client.okhttp.OpenAIOkHttpClient
                import com.openai.models.ChatModel
                import com.openai.models.chat.completions.ChatCompletion
                import com.openai.models.chat.completions.ChatCompletionCreateParams

                fun main() {
                    // Configures using the `OPENAI_API_KEY`, `OPENAI_ORG_ID`, `OPENAI_PROJECT_ID`, `OPENAI_WEBHOOK_SECRET` and `OPENAI_BASE_URL` environment variables
                    val client: OpenAIClient = OpenAIOkHttpClient.fromEnv()

                    val params: ChatCompletionCreateParams = ChatCompletionCreateParams.builder()
                        .addDeveloperMessage("string")
                        .model(ChatModel.GPT_4_1)
                        .build()
                    val chatCompletion: ChatCompletion = client.chat().completions().create(params)
                }
              ruby: >-
                require "openai"


                openai = OpenAI::Client.new(
                  api_key: ENV["OPENAI_API_KEY"] # This is the default and can be omitted
                )


                chat_completion = openai.chat.completions.create(messages: [{content: "string", role:
                :developer}], model: :"gpt-4.1")


                puts(chat_completion)
            response: |
              {
                "id": "chatcmpl-123",
                "object": "chat.completion",
                "created": 1702685778,
                "model": "gpt-4o-mini",
                "choices": [
                  {
                    "index": 0,
                    "message": {
                      "role": "assistant",
                      "content": "Hello! How can I assist you today?"
                    },
                    "logprobs": {
                      "content": [
                        {
                          "token": "Hello",
                          "logprob": -0.31725305,
                          "bytes": [72, 101, 108, 108, 111],
                          "top_logprobs": [
                            {
                              "token": "Hello",
                              "logprob": -0.31725305,
                              "bytes": [72, 101, 108, 108, 111]
                            },
                            {
                              "token": "Hi",
                              "logprob": -1.3190403,
                              "bytes": [72, 105]
                            }
                          ]
                        },
                        {
                          "token": "!",
                          "logprob": -0.02380986,
                          "bytes": [
                            33
                          ],
                          "top_logprobs": [
                            {
                              "token": "!",
                              "logprob": -0.02380986,
                              "bytes": [33]
                            },
                            {
                              "token": " there",
                              "logprob": -3.787621,
                              "bytes": [32, 116, 104, 101, 114, 101]
                            }
                          ]
                        },
                        {
                          "token": " How",
                          "logprob": -0.000054669687,
                          "bytes": [32, 72, 111, 119],
                          "top_logprobs": [
                            {
                              "token": " How",
                              "logprob": -0.000054669687,
                              "bytes": [32, 72, 111, 119]
                            },
                            {
                              "token": "<|end|>",
                              "logprob": -10.953937,
                              "bytes": null
                            }
                          ]
                        },
                        {
                          "token": " can",
                          "logprob": -0.015801601,
                          "bytes": [32, 99, 97, 110],
                          "top_logprobs": [
                            {
                              "token": " can",
                              "logprob": -0.015801601,
                              "bytes": [32, 99, 97, 110]
                            },
                            {
                              "token": " may",
                              "logprob": -4.161023,
                              "bytes": [32, 109, 97, 121]
                            }
                          ]
                        },
                        {
                          "token": " I",
                          "logprob": -3.7697225e-6,
                          "bytes": [
                            32,
                            73
                          ],
                          "top_logprobs": [
                            {
                              "token": " I",
                              "logprob": -3.7697225e-6,
                              "bytes": [32, 73]
                            },
                            {
                              "token": " assist",
                              "logprob": -13.596657,
                              "bytes": [32, 97, 115, 115, 105, 115, 116]
                            }
                          ]
                        },
                        {
                          "token": " assist",
                          "logprob": -0.04571125,
                          "bytes": [32, 97, 115, 115, 105, 115, 116],
                          "top_logprobs": [
                            {
                              "token": " assist",
                              "logprob": -0.04571125,
                              "bytes": [32, 97, 115, 115, 105, 115, 116]
                            },
                            {
                              "token": " help",
                              "logprob": -3.1089056,
                              "bytes": [32, 104, 101, 108, 112]
                            }
                          ]
                        },
                        {
                          "token": " you",
                          "logprob": -5.4385737e-6,
                          "bytes": [32, 121, 111, 117],
                          "top_logprobs": [
                            {
                              "token": " you",
                              "logprob": -5.4385737e-6,
                              "bytes": [32, 121, 111, 117]
                            },
                            {
                              "token": " today",
                              "logprob": -12.807695,
                              "bytes": [32, 116, 111, 100, 97, 121]
                            }
                          ]
                        },
                        {
                          "token": " today",
                          "logprob": -0.0040071653,
                          "bytes": [32, 116, 111, 100, 97, 121],
                          "top_logprobs": [
                            {
                              "token": " today",
                              "logprob": -0.0040071653,
                              "bytes": [32, 116, 111, 100, 97, 121]
                            },
                            {
                              "token": "?",
                              "logprob": -5.5247097,
                              "bytes": [63]
                            }
                          ]
                        },
                        {
                          "token": "?",
                          "logprob": -0.0008108172,
                          "bytes": [63],
                          "top_logprobs": [
                            {
                              "token": "?",
                              "logprob": -0.0008108172,
                              "bytes": [63]
                            },
                            {
                              "token": "?\n",
                              "logprob": -7.184561,
                              "bytes": [63, 10]
                            }
                          ]
                        }
                      ]
                    },
                    "finish_reason": "stop"
                  }
                ],
                "usage": {
                  "prompt_tokens": 9,
                  "completion_tokens": 9,
                  "total_tokens": 18,
                  "completion_tokens_details": {
                    "reasoning_tokens": 0,
                    "accepted_prediction_tokens": 0,
                    "rejected_prediction_tokens": 0
                  }
                },
                "system_fingerprint": null
              }
  /completions:
    post:
      operationId: createCompletion
      tags:
        - Completions
      summary: Creates a completion for the provided prompt and parameters.
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/CreateCompletionRequest'
      responses:
        '200':
          description: OK
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/CreateCompletionResponse'
      x-oaiMeta:
        name: Create completion
        group: completions
        returns: >
          Returns a [completion](/docs/api-reference/completions/object) object, or a sequence of completion
          objects if the request is streamed.
        legacy: true
        examples:
          - title: No streaming
            request:
              curl: |
                curl https://api.openai.com/v1/completions \
                  -H "Content-Type: application/json" \
                  -H "Authorization: Bearer $OPENAI_API_KEY" \
                  -d '{
                    "model": "VAR_completion_model_id",
                    "prompt": "Say this is a test",
                    "max_tokens": 7,
                    "temperature": 0
                  }'
              python: |-
                import os
                from openai import OpenAI

                client = OpenAI(
                    api_key=os.environ.get("OPENAI_API_KEY"),  # This is the default and can be omitted
                )
                completion = client.completions.create(
                    model="string",
                    prompt="This is a test.",
                )
                print(completion)
              node.js: >-
                import OpenAI from 'openai';


                const client = new OpenAI({
                  apiKey: process.env['OPENAI_API_KEY'], // This is the default and can be omitted
                });


                const completion = await client.completions.create({ model: 'string', prompt: 'This is a
                test.' });


                console.log(completion);
              go: |
                package main

                import (
                  "context"
                  "fmt"

                  "github.com/openai/openai-go"
                  "github.com/openai/openai-go/option"
                )

                func main() {
                  client := openai.NewClient(
                    option.WithAPIKey("My API Key"), // defaults to os.LookupEnv("OPENAI_API_KEY")
                  )
                  completion, err := client.Completions.New(context.TODO(), openai.CompletionNewParams{
                    Model: openai.CompletionNewParamsModelGPT3_5TurboInstruct,
                    Prompt: openai.CompletionNewParamsPromptUnion{
                      OfString: openai.String("This is a test."),
                    },
                  })
                  if err != nil {
                    panic(err.Error())
                  }
                  fmt.Printf("%+v\n", completion)
                }
              java: |-
                package com.openai.example;

                import com.openai.client.OpenAIClient;
                import com.openai.client.okhttp.OpenAIOkHttpClient;
                import com.openai.models.completions.Completion;
                import com.openai.models.completions.CompletionCreateParams;

                public final class Main {
                    private Main() {}

                    public static void main(String[] args) {
                        // Configures using the `OPENAI_API_KEY`, `OPENAI_ORG_ID`, `OPENAI_PROJECT_ID`, `OPENAI_WEBHOOK_SECRET` and `OPENAI_BASE_URL` environment variables
                        OpenAIClient client = OpenAIOkHttpClient.fromEnv();

                        CompletionCreateParams params = CompletionCreateParams.builder()
                            .model(CompletionCreateParams.Model.GPT_3_5_TURBO_INSTRUCT)
                            .prompt("This is a test.")
                            .build();
                        Completion completion = client.completions().create(params);
                    }
                }
              kotlin: |-
                package com.openai.example

                import com.openai.client.OpenAIClient
                import com.openai.client.okhttp.OpenAIOkHttpClient
                import com.openai.models.completions.Completion
                import com.openai.models.completions.CompletionCreateParams

                fun main() {
                    // Configures using the `OPENAI_API_KEY`, `OPENAI_ORG_ID`, `OPENAI_PROJECT_ID`, `OPENAI_WEBHOOK_SECRET` and `OPENAI_BASE_URL` environment variables
                    val client: OpenAIClient = OpenAIOkHttpClient.fromEnv()

                    val params: CompletionCreateParams = CompletionCreateParams.builder()
                        .model(CompletionCreateParams.Model.GPT_3_5_TURBO_INSTRUCT)
                        .prompt("This is a test.")
                        .build()
                    val completion: Completion = client.completions().create(params)
                }
              ruby: >-
                require "openai"


                openai = OpenAI::Client.new(
                  api_key: ENV["OPENAI_API_KEY"] # This is the default and can be omitted
                )


                completion = openai.completions.create(model: :"gpt-3.5-turbo-instruct", prompt: "This is a
                test.")


                puts(completion)
            response: |
              {
                "id": "cmpl-uqkvlQyYK7bGYrRHQ0eXlWi7",
                "object": "text_completion",
                "created": 1589478378,
                "model": "VAR_completion_model_id",
                "system_fingerprint": "fp_44709d6fcb",
                "choices": [
                  {
                    "text": "\n\nThis is indeed a test",
                    "index": 0,
                    "logprobs": null,
                    "finish_reason": "length"
                  }
                ],
                "usage": {
                  "prompt_tokens": 5,
                  "completion_tokens": 7,
                  "total_tokens": 12
                }
              }
          - title: Streaming
            request:
              curl: |
                curl https://api.openai.com/v1/completions \
                  -H "Content-Type: application/json" \
                  -H "Authorization: Bearer $OPENAI_API_KEY" \
                  -d '{
                    "model": "VAR_completion_model_id",
                    "prompt": "Say this is a test",
                    "max_tokens": 7,
                    "temperature": 0,
                    "stream": true
                  }'
              python: |-
                import os
                from openai import OpenAI

                client = OpenAI(
                    api_key=os.environ.get("OPENAI_API_KEY"),  # This is the default and can be omitted
                )
                completion = client.completions.create(
                    model="string",
                    prompt="This is a test.",
                )
                print(completion)
              node.js: >-
                import OpenAI from 'openai';


                const client = new OpenAI({
                  apiKey: process.env['OPENAI_API_KEY'], // This is the default and can be omitted
                });


                const completion = await client.completions.create({ model: 'string', prompt: 'This is a
                test.' });


                console.log(completion);
              go: |
                package main

                import (
                  "context"
                  "fmt"

                  "github.com/openai/openai-go"
                  "github.com/openai/openai-go/option"
                )

                func main() {
                  client := openai.NewClient(
                    option.WithAPIKey("My API Key"), // defaults to os.LookupEnv("OPENAI_API_KEY")
                  )
                  completion, err := client.Completions.New(context.TODO(), openai.CompletionNewParams{
                    Model: openai.CompletionNewParamsModelGPT3_5TurboInstruct,
                    Prompt: openai.CompletionNewParamsPromptUnion{
                      OfString: openai.String("This is a test."),
                    },
                  })
                  if err != nil {
                    panic(err.Error())
                  }
                  fmt.Printf("%+v\n", completion)
                }
              java: |-
                package com.openai.example;

                import com.openai.client.OpenAIClient;
                import com.openai.client.okhttp.OpenAIOkHttpClient;
                import com.openai.models.completions.Completion;
                import com.openai.models.completions.CompletionCreateParams;

                public final class Main {
                    private Main() {}

                    public static void main(String[] args) {
                        // Configures using the `OPENAI_API_KEY`, `OPENAI_ORG_ID`, `OPENAI_PROJECT_ID`, `OPENAI_WEBHOOK_SECRET` and `OPENAI_BASE_URL` environment variables
                        OpenAIClient client = OpenAIOkHttpClient.fromEnv();

                        CompletionCreateParams params = CompletionCreateParams.builder()
                            .model(CompletionCreateParams.Model.GPT_3_5_TURBO_INSTRUCT)
                            .prompt("This is a test.")
                            .build();
                        Completion completion = client.completions().create(params);
                    }
                }
              kotlin: |-
                package com.openai.example

                import com.openai.client.OpenAIClient
                import com.openai.client.okhttp.OpenAIOkHttpClient
                import com.openai.models.completions.Completion
                import com.openai.models.completions.CompletionCreateParams

                fun main() {
                    // Configures using the `OPENAI_API_KEY`, `OPENAI_ORG_ID`, `OPENAI_PROJECT_ID`, `OPENAI_WEBHOOK_SECRET` and `OPENAI_BASE_URL` environment variables
                    val client: OpenAIClient = OpenAIOkHttpClient.fromEnv()

                    val params: CompletionCreateParams = CompletionCreateParams.builder()
                        .model(CompletionCreateParams.Model.GPT_3_5_TURBO_INSTRUCT)
                        .prompt("This is a test.")
                        .build()
                    val completion: Completion = client.completions().create(params)
                }
              ruby: >-
                require "openai"


                openai = OpenAI::Client.new(
                  api_key: ENV["OPENAI_API_KEY"] # This is the default and can be omitted
                )


                completion = openai.completions.create(model: :"gpt-3.5-turbo-instruct", prompt: "This is a
                test.")


                puts(completion)
            response: |
              {
                "id": "cmpl-7iA7iJjj8V2zOkCGvWF2hAkDWBQZe",
                "object": "text_completion",
                "created": 1690759702,
                "choices": [
                  {
                    "text": "This",
                    "index": 0,
                    "logprobs": null,
                    "finish_reason": null
                  }
                ],
                "model": "gpt-3.5-turbo-instruct"
                "system_fingerprint": "fp_44709d6fcb",
              }
  /embeddings:
    post:
      operationId: createEmbedding
      tags:
        - Embeddings
      summary: Creates an embedding vector representing the input text.
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/CreateEmbeddingRequest'
      responses:
        '200':
          description: OK
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/CreateEmbeddingResponse'
      x-oaiMeta:
        name: Create embeddings
        group: embeddings
        returns: A list of [embedding](/docs/api-reference/embeddings/object) objects.
        examples:
          response: |
            {
              "object": "list",
              "data": [
                {
                  "object": "embedding",
                  "embedding": [
                    0.0023064255,
                    -0.009327292,
                    .... (1536 floats total for ada-002)
                    -0.0028842222,
                  ],
                  "index": 0
                }
              ],
              "model": "text-embedding-ada-002",
              "usage": {
                "prompt_tokens": 8,
                "total_tokens": 8
              }
            }
          request:
            curl: |
              curl https://api.openai.com/v1/embeddings \
                -H "Authorization: Bearer $OPENAI_API_KEY" \
                -H "Content-Type: application/json" \
                -d '{
                  "input": "The food was delicious and the waiter...",
                  "model": "text-embedding-ada-002",
                  "encoding_format": "float"
                }'
            python: |-
              import os
              from openai import OpenAI

              client = OpenAI(
                  api_key=os.environ.get("OPENAI_API_KEY"),  # This is the default and can be omitted
              )
              create_embedding_response = client.embeddings.create(
                  input="The quick brown fox jumped over the lazy dog",
                  model="text-embedding-3-small",
              )
              print(create_embedding_response.data)
            node.js: |-
              import OpenAI from 'openai';

              const client = new OpenAI({
                apiKey: process.env['OPENAI_API_KEY'], // This is the default and can be omitted
              });

              const createEmbeddingResponse = await client.embeddings.create({
                input: 'The quick brown fox jumped over the lazy dog',
                model: 'text-embedding-3-small',
              });

              console.log(createEmbeddingResponse.data);
            csharp: >
              using System;


              using OpenAI.Embeddings;


              EmbeddingClient client = new(
                  model: "text-embedding-3-small",
                  apiKey: Environment.GetEnvironmentVariable("OPENAI_API_KEY")
              );


              OpenAIEmbedding embedding = client.GenerateEmbedding(input: "The quick brown fox jumped over the
              lazy dog");

              ReadOnlyMemory<float> vector = embedding.ToFloats();


              for (int i = 0; i < vector.Length; i++)

              {
                  Console.WriteLine($"  [{i,4}] = {vector.Span[i]}");
              }
            go: |
              package main

              import (
                "context"
                "fmt"

                "github.com/openai/openai-go"
                "github.com/openai/openai-go/option"
              )

              func main() {
                client := openai.NewClient(
                  option.WithAPIKey("My API Key"), // defaults to os.LookupEnv("OPENAI_API_KEY")
                )
                createEmbeddingResponse, err := client.Embeddings.New(context.TODO(), openai.EmbeddingNewParams{
                  Input: openai.EmbeddingNewParamsInputUnion{
                    OfString: openai.String("The quick brown fox jumped over the lazy dog"),
                  },
                  Model: openai.EmbeddingModelTextEmbeddingAda002,
                })
                if err != nil {
                  panic(err.Error())
                }
                fmt.Printf("%+v\n", createEmbeddingResponse.Data)
              }
            java: |-
              package com.openai.example;

              import com.openai.client.OpenAIClient;
              import com.openai.client.okhttp.OpenAIOkHttpClient;
              import com.openai.models.embeddings.CreateEmbeddingResponse;
              import com.openai.models.embeddings.EmbeddingCreateParams;
              import com.openai.models.embeddings.EmbeddingModel;

              public final class Main {
                  private Main() {}

                  public static void main(String[] args) {
                      // Configures using the `OPENAI_API_KEY`, `OPENAI_ORG_ID`, `OPENAI_PROJECT_ID`, `OPENAI_WEBHOOK_SECRET` and `OPENAI_BASE_URL` environment variables
                      OpenAIClient client = OpenAIOkHttpClient.fromEnv();

                      EmbeddingCreateParams params = EmbeddingCreateParams.builder()
                          .input("The quick brown fox jumped over the lazy dog")
                          .model(EmbeddingModel.TEXT_EMBEDDING_ADA_002)
                          .build();
                      CreateEmbeddingResponse createEmbeddingResponse = client.embeddings().create(params);
                  }
              }
            kotlin: |-
              package com.openai.example

              import com.openai.client.OpenAIClient
              import com.openai.client.okhttp.OpenAIOkHttpClient
              import com.openai.models.embeddings.CreateEmbeddingResponse
              import com.openai.models.embeddings.EmbeddingCreateParams
              import com.openai.models.embeddings.EmbeddingModel

              fun main() {
                  // Configures using the `OPENAI_API_KEY`, `OPENAI_ORG_ID`, `OPENAI_PROJECT_ID`, `OPENAI_WEBHOOK_SECRET` and `OPENAI_BASE_URL` environment variables
                  val client: OpenAIClient = OpenAIOkHttpClient.fromEnv()

                  val params: EmbeddingCreateParams = EmbeddingCreateParams.builder()
                      .input("The quick brown fox jumped over the lazy dog")
                      .model(EmbeddingModel.TEXT_EMBEDDING_ADA_002)
                      .build()
                  val createEmbeddingResponse: CreateEmbeddingResponse = client.embeddings().create(params)
              }
            ruby: |-
              require "openai"

              openai = OpenAI::Client.new(
                api_key: ENV["OPENAI_API_KEY"] # This is the default and can be omitted
              )

              create_embedding_response = openai.embeddings.create(
                input: "The quick brown fox jumped over the lazy dog",
                model: :"text-embedding-ada-002"
              )

              puts(create_embedding_response)
  /models:
    get:
      operationId: listModels
      tags:
        - Models
      summary: >-
        Lists the currently available models, and provides basic information about each one such as the owner
        and availability.
      responses:
        '200':
          description: OK
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ListModelsResponse'
      x-oaiMeta:
        name: List models
        group: models
        returns: A list of [model](/docs/api-reference/models/object) objects.
        examples:
          response: |
            {
              "object": "list",
              "data": [
                {
                  "id": "model-id-0",
                  "object": "model",
                  "created": 1686935002,
                  "owned_by": "organization-owner"
                },
                {
                  "id": "model-id-1",
                  "object": "model",
                  "created": 1686935002,
                  "owned_by": "organization-owner",
                },
                {
                  "id": "model-id-2",
                  "object": "model",
                  "created": 1686935002,
                  "owned_by": "openai"
                },
              ],
              "object": "list"
            }
          request:
            curl: |
              curl https://api.openai.com/v1/models \
                -H "Authorization: Bearer $OPENAI_API_KEY"
            python: |-
              import os
              from openai import OpenAI

              client = OpenAI(
                  api_key=os.environ.get("OPENAI_API_KEY"),  # This is the default and can be omitted
              )
              page = client.models.list()
              page = page.data[0]
              print(page.id)
            node.js: |-
              import OpenAI from 'openai';

              const client = new OpenAI({
                apiKey: process.env['OPENAI_API_KEY'], // This is the default and can be omitted
              });

              // Automatically fetches more pages as needed.
              for await (const model of client.models.list()) {
                console.log(model.id);
              }
            csharp: |
              using System;

              using OpenAI.Models;

              OpenAIModelClient client = new(
                  apiKey: Environment.GetEnvironmentVariable("OPENAI_API_KEY")
              );

              foreach (var model in client.GetModels().Value)
              {
                  Console.WriteLine(model.Id);
              }
            go: |
              package main

              import (
                "context"
                "fmt"

                "github.com/openai/openai-go"
                "github.com/openai/openai-go/option"
              )

              func main() {
                client := openai.NewClient(
                  option.WithAPIKey("My API Key"), // defaults to os.LookupEnv("OPENAI_API_KEY")
                )
                page, err := client.Models.List(context.TODO())
                if err != nil {
                  panic(err.Error())
                }
                fmt.Printf("%+v\n", page)
              }
            java: |-
              package com.openai.example;

              import com.openai.client.OpenAIClient;
              import com.openai.client.okhttp.OpenAIOkHttpClient;
              import com.openai.models.models.ModelListPage;
              import com.openai.models.models.ModelListParams;

              public final class Main {
                  private Main() {}

                  public static void main(String[] args) {
                      // Configures using the `OPENAI_API_KEY`, `OPENAI_ORG_ID`, `OPENAI_PROJECT_ID`, `OPENAI_WEBHOOK_SECRET` and `OPENAI_BASE_URL` environment variables
                      OpenAIClient client = OpenAIOkHttpClient.fromEnv();

                      ModelListPage page = client.models().list();
                  }
              }
            kotlin: |-
              package com.openai.example

              import com.openai.client.OpenAIClient
              import com.openai.client.okhttp.OpenAIOkHttpClient
              import com.openai.models.models.ModelListPage
              import com.openai.models.models.ModelListParams

              fun main() {
                  // Configures using the `OPENAI_API_KEY`, `OPENAI_ORG_ID`, `OPENAI_PROJECT_ID`, `OPENAI_WEBHOOK_SECRET` and `OPENAI_BASE_URL` environment variables
                  val client: OpenAIClient = OpenAIOkHttpClient.fromEnv()

                  val page: ModelListPage = client.models().list()
              }
            ruby: |-
              require "openai"

              openai = OpenAI::Client.new(
                api_key: ENV["OPENAI_API_KEY"] # This is the default and can be omitted
              )

              page = openai.models.list

              puts(page)
  /models/{model}:
    get:
      operationId: retrieveModel
      tags:
        - Models
      summary: >-
        Retrieves a model instance, providing basic information about the model such as the owner and
        permissioning.
      parameters:
        - in: path
          name: model
          required: true
          schema:
            type: string
            example: gpt-4o-mini
          description: The ID of the model to use for this request
      responses:
        '200':
          description: OK
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Model'
      x-oaiMeta:
        name: Retrieve model
        group: models
        returns: The [model](/docs/api-reference/models/object) object matching the specified ID.
        examples:
          response: |
            {
              "id": "VAR_chat_model_id",
              "object": "model",
              "created": 1686935002,
              "owned_by": "openai"
            }
          request:
            curl: |
              curl https://api.openai.com/v1/models/VAR_chat_model_id \
                -H "Authorization: Bearer $OPENAI_API_KEY"
            python: |-
              import os
              from openai import OpenAI

              client = OpenAI(
                  api_key=os.environ.get("OPENAI_API_KEY"),  # This is the default and can be omitted
              )
              model = client.models.retrieve(
                  "gpt-4o-mini",
              )
              print(model.id)
            node.js: |-
              import OpenAI from 'openai';

              const client = new OpenAI({
                apiKey: process.env['OPENAI_API_KEY'], // This is the default and can be omitted
              });

              const model = await client.models.retrieve('gpt-4o-mini');

              console.log(model.id);
            csharp: |
              using System;
              using System.ClientModel;

              using OpenAI.Models;

                OpenAIModelClient client = new(
                  apiKey: Environment.GetEnvironmentVariable("OPENAI_API_KEY")
              );

              ClientResult<OpenAIModel> model = client.GetModel("babbage-002");
              Console.WriteLine(model.Value.Id);
            go: |
              package main

              import (
                "context"
                "fmt"

                "github.com/openai/openai-go"
                "github.com/openai/openai-go/option"
              )

              func main() {
                client := openai.NewClient(
                  option.WithAPIKey("My API Key"), // defaults to os.LookupEnv("OPENAI_API_KEY")
                )
                model, err := client.Models.Get(context.TODO(), "gpt-4o-mini")
                if err != nil {
                  panic(err.Error())
                }
                fmt.Printf("%+v\n", model.ID)
              }
            java: |-
              package com.openai.example;

              import com.openai.client.OpenAIClient;
              import com.openai.client.okhttp.OpenAIOkHttpClient;
              import com.openai.models.models.Model;
              import com.openai.models.models.ModelRetrieveParams;

              public final class Main {
                  private Main() {}

                  public static void main(String[] args) {
                      // Configures using the `OPENAI_API_KEY`, `OPENAI_ORG_ID`, `OPENAI_PROJECT_ID`, `OPENAI_WEBHOOK_SECRET` and `OPENAI_BASE_URL` environment variables
                      OpenAIClient client = OpenAIOkHttpClient.fromEnv();

                      Model model = client.models().retrieve("gpt-4o-mini");
                  }
              }
            kotlin: |-
              package com.openai.example

              import com.openai.client.OpenAIClient
              import com.openai.client.okhttp.OpenAIOkHttpClient
              import com.openai.models.models.Model
              import com.openai.models.models.ModelRetrieveParams

              fun main() {
                  // Configures using the `OPENAI_API_KEY`, `OPENAI_ORG_ID`, `OPENAI_PROJECT_ID`, `OPENAI_WEBHOOK_SECRET` and `OPENAI_BASE_URL` environment variables
                  val client: OpenAIClient = OpenAIOkHttpClient.fromEnv()

                  val model: Model = client.models().retrieve("gpt-4o-mini")
              }
            ruby: |-
              require "openai"

              openai = OpenAI::Client.new(
                api_key: ENV["OPENAI_API_KEY"] # This is the default and can be omitted
              )

              model = openai.models.retrieve("gpt-4o-mini")

              puts(model)
  /moderations:
    post:
      operationId: createModeration
      tags:
        - Moderations
      summary: |
        Classifies if text and/or image inputs are potentially harmful. Learn
        more in the [moderation guide](/docs/guides/moderation).
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/CreateModerationRequest'
      responses:
        '200':
          description: OK
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/CreateModerationResponse'
      x-oaiMeta:
        name: Create moderation
        group: moderations
        returns: A [moderation](/docs/api-reference/moderations/object) object.
        examples:
          - title: Single string
            request:
              curl: |
                curl https://api.openai.com/v1/moderations \
                  -H "Content-Type: application/json" \
                  -H "Authorization: Bearer $OPENAI_API_KEY" \
                  -d '{
                    "input": "I want to kill them."
                  }'
              python: |-
                import os
                from openai import OpenAI

                client = OpenAI(
                    api_key=os.environ.get("OPENAI_API_KEY"),  # This is the default and can be omitted
                )
                moderation = client.moderations.create(
                    input="I want to kill them.",
                )
                print(moderation.id)
              node.js: |-
                import OpenAI from 'openai';

                const client = new OpenAI({
                  apiKey: process.env['OPENAI_API_KEY'], // This is the default and can be omitted
                });

                const moderation = await client.moderations.create({ input: 'I want to kill them.' });

                console.log(moderation.id);
              csharp: |
                using System;
                using System.ClientModel;

                using OpenAI.Moderations;

                ModerationClient client = new(
                    model: "omni-moderation-latest",
                    apiKey: Environment.GetEnvironmentVariable("OPENAI_API_KEY")
                );

                ClientResult<ModerationResult> moderation = client.ClassifyText("I want to kill them.");
              go: |
                package main

                import (
                  "context"
                  "fmt"

                  "github.com/openai/openai-go"
                  "github.com/openai/openai-go/option"
                )

                func main() {
                  client := openai.NewClient(
                    option.WithAPIKey("My API Key"), // defaults to os.LookupEnv("OPENAI_API_KEY")
                  )
                  moderation, err := client.Moderations.New(context.TODO(), openai.ModerationNewParams{
                    Input: openai.ModerationNewParamsInputUnion{
                      OfString: openai.String("I want to kill them."),
                    },
                  })
                  if err != nil {
                    panic(err.Error())
                  }
                  fmt.Printf("%+v\n", moderation.ID)
                }
              java: |-
                package com.openai.example;

                import com.openai.client.OpenAIClient;
                import com.openai.client.okhttp.OpenAIOkHttpClient;
                import com.openai.models.moderations.ModerationCreateParams;
                import com.openai.models.moderations.ModerationCreateResponse;

                public final class Main {
                    private Main() {}

                    public static void main(String[] args) {
                        // Configures using the `OPENAI_API_KEY`, `OPENAI_ORG_ID`, `OPENAI_PROJECT_ID`, `OPENAI_WEBHOOK_SECRET` and `OPENAI_BASE_URL` environment variables
                        OpenAIClient client = OpenAIOkHttpClient.fromEnv();

                        ModerationCreateParams params = ModerationCreateParams.builder()
                            .input("I want to kill them.")
                            .build();
                        ModerationCreateResponse moderation = client.moderations().create(params);
                    }
                }
              kotlin: |-
                package com.openai.example

                import com.openai.client.OpenAIClient
                import com.openai.client.okhttp.OpenAIOkHttpClient
                import com.openai.models.moderations.ModerationCreateParams
                import com.openai.models.moderations.ModerationCreateResponse

                fun main() {
                    // Configures using the `OPENAI_API_KEY`, `OPENAI_ORG_ID`, `OPENAI_PROJECT_ID`, `OPENAI_WEBHOOK_SECRET` and `OPENAI_BASE_URL` environment variables
                    val client: OpenAIClient = OpenAIOkHttpClient.fromEnv()

                    val params: ModerationCreateParams = ModerationCreateParams.builder()
                        .input("I want to kill them.")
                        .build()
                    val moderation: ModerationCreateResponse = client.moderations().create(params)
                }
              ruby: |-
                require "openai"

                openai = OpenAI::Client.new(
                  api_key: ENV["OPENAI_API_KEY"] # This is the default and can be omitted
                )

                moderation = openai.moderations.create(input: "I want to kill them.")

                puts(moderation)
            response: |
              {
                "id": "modr-AB8CjOTu2jiq12hp1AQPfeqFWaORR",
                "model": "text-moderation-007",
                "results": [
                  {
                    "flagged": true,
                    "categories": {
                      "sexual": false,
                      "hate": false,
                      "harassment": true,
                      "self-harm": false,
                      "sexual/minors": false,
                      "hate/threatening": false,
                      "violence/graphic": false,
                      "self-harm/intent": false,
                      "self-harm/instructions": false,
                      "harassment/threatening": true,
                      "violence": true
                    },
                    "category_scores": {
                      "sexual": 0.000011726012417057063,
                      "hate": 0.22706663608551025,
                      "harassment": 0.5215635299682617,
                      "self-harm": 2.227119921371923e-6,
                      "sexual/minors": 7.107352217872176e-8,
                      "hate/threatening": 0.023547329008579254,
                      "violence/graphic": 0.00003391829886822961,
                      "self-harm/intent": 1.646940972932498e-6,
                      "self-harm/instructions": 1.1198755256458526e-9,
                      "harassment/threatening": 0.5694745779037476,
                      "violence": 0.9971134662628174
                    }
                  }
                ]
              }
          - title: Image and text
            request:
              curl: |
                curl https://api.openai.com/v1/moderations \
                  -X POST \
                  -H "Content-Type: application/json" \
                  -H "Authorization: Bearer $OPENAI_API_KEY" \
                  -d '{
                    "model": "omni-moderation-latest",
                    "input": [
                      { "type": "text", "text": "...text to classify goes here..." },
                      {
                        "type": "image_url",
                        "image_url": {
                          "url": "https://example.com/image.png"
                        }
                      }
                    ]
                  }'
              python: |-
                import os
                from openai import OpenAI

                client = OpenAI(
                    api_key=os.environ.get("OPENAI_API_KEY"),  # This is the default and can be omitted
                )
                moderation = client.moderations.create(
                    input="I want to kill them.",
                )
                print(moderation.id)
              node.js: |-
                import OpenAI from 'openai';

                const client = new OpenAI({
                  apiKey: process.env['OPENAI_API_KEY'], // This is the default and can be omitted
                });

                const moderation = await client.moderations.create({ input: 'I want to kill them.' });

                console.log(moderation.id);
              go: |
                package main

                import (
                  "context"
                  "fmt"

                  "github.com/openai/openai-go"
                  "github.com/openai/openai-go/option"
                )

                func main() {
                  client := openai.NewClient(
                    option.WithAPIKey("My API Key"), // defaults to os.LookupEnv("OPENAI_API_KEY")
                  )
                  moderation, err := client.Moderations.New(context.TODO(), openai.ModerationNewParams{
                    Input: openai.ModerationNewParamsInputUnion{
                      OfString: openai.String("I want to kill them."),
                    },
                  })
                  if err != nil {
                    panic(err.Error())
                  }
                  fmt.Printf("%+v\n", moderation.ID)
                }
              java: |-
                package com.openai.example;

                import com.openai.client.OpenAIClient;
                import com.openai.client.okhttp.OpenAIOkHttpClient;
                import com.openai.models.moderations.ModerationCreateParams;
                import com.openai.models.moderations.ModerationCreateResponse;

                public final class Main {
                    private Main() {}

                    public static void main(String[] args) {
                        // Configures using the `OPENAI_API_KEY`, `OPENAI_ORG_ID`, `OPENAI_PROJECT_ID`, `OPENAI_WEBHOOK_SECRET` and `OPENAI_BASE_URL` environment variables
                        OpenAIClient client = OpenAIOkHttpClient.fromEnv();

                        ModerationCreateParams params = ModerationCreateParams.builder()
                            .input("I want to kill them.")
                            .build();
                        ModerationCreateResponse moderation = client.moderations().create(params);
                    }
                }
              kotlin: |-
                package com.openai.example

                import com.openai.client.OpenAIClient
                import com.openai.client.okhttp.OpenAIOkHttpClient
                import com.openai.models.moderations.ModerationCreateParams
                import com.openai.models.moderations.ModerationCreateResponse

                fun main() {
                    // Configures using the `OPENAI_API_KEY`, `OPENAI_ORG_ID`, `OPENAI_PROJECT_ID`, `OPENAI_WEBHOOK_SECRET` and `OPENAI_BASE_URL` environment variables
                    val client: OpenAIClient = OpenAIOkHttpClient.fromEnv()

                    val params: ModerationCreateParams = ModerationCreateParams.builder()
                        .input("I want to kill them.")
                        .build()
                    val moderation: ModerationCreateResponse = client.moderations().create(params)
                }
              ruby: |-
                require "openai"

                openai = OpenAI::Client.new(
                  api_key: ENV["OPENAI_API_KEY"] # This is the default and can be omitted
                )

                moderation = openai.moderations.create(input: "I want to kill them.")

                puts(moderation)
            response: |
              {
                "id": "modr-0d9740456c391e43c445bf0f010940c7",
                "model": "omni-moderation-latest",
                "results": [
                  {
                    "flagged": true,
                    "categories": {
                      "harassment": true,
                      "harassment/threatening": true,
                      "sexual": false,
                      "hate": false,
                      "hate/threatening": false,
                      "illicit": false,
                      "illicit/violent": false,
                      "self-harm/intent": false,
                      "self-harm/instructions": false,
                      "self-harm": false,
                      "sexual/minors": false,
                      "violence": true,
                      "violence/graphic": true
                    },
                    "category_scores": {
                      "harassment": 0.8189693396524255,
                      "harassment/threatening": 0.804985420696006,
                      "sexual": 1.573112165348997e-6,
                      "hate": 0.007562942636942845,
                      "hate/threatening": 0.004208854591835476,
                      "illicit": 0.030535955153511665,
                      "illicit/violent": 0.008925306722380033,
                      "self-harm/intent": 0.00023023930975076432,
                      "self-harm/instructions": 0.0002293869201073356,
                      "self-harm": 0.012598046106750154,
                      "sexual/minors": 2.212566909570261e-8,
                      "violence": 0.9999992735124786,
                      "violence/graphic": 0.843064871157054
                    },
                    "category_applied_input_types": {
                      "harassment": [
                        "text"
                      ],
                      "harassment/threatening": [
                        "text"
                      ],
                      "sexual": [
                        "text",
                        "image"
                      ],
                      "hate": [
                        "text"
                      ],
                      "hate/threatening": [
                        "text"
                      ],
                      "illicit": [
                        "text"
                      ],
                      "illicit/violent": [
                        "text"
                      ],
                      "self-harm/intent": [
                        "text",
                        "image"
                      ],
                      "self-harm/instructions": [
                        "text",
                        "image"
                      ],
                      "self-harm": [
                        "text",
                        "image"
                      ],
                      "sexual/minors": [
                        "text"
                      ],
                      "violence": [
                        "text",
                        "image"
                      ],
                      "violence/graphic": [
                        "text",
                        "image"
                      ]
                    }
                  }
                ]
              }
components:
  schemas:
    Metadata:
      type: object
      description: |
        Set of 16 key-value pairs that can be attached to an object. This can be
        useful for storing additional information about the object in a structured
        format, and querying for objects via API or the dashboard. 

        Keys are strings with a maximum length of 64 characters. Values are strings
        with a maximum length of 512 characters.
      additionalProperties:
        type: string
      x-oaiTypeLabel: map
      nullable: true
    ServiceTier:
      type: string
      description: |
        Specifies the processing type used for serving the request.
          - If set to 'auto', then the request will be processed with the service tier configured in the Project settings. Unless otherwise configured, the Project will use 'default'.
          - If set to 'default', then the requset will be processed with the standard pricing and performance for the selected model.
          - If set to '[flex](/docs/guides/flex-processing)' or 'priority', then the request will be processed with the corresponding service tier. [Contact sales](https://openai.com/contact-sales) to learn more about Priority processing.
          - When not set, the default behavior is 'auto'.

          When the `service_tier` parameter is set, the response body will include the `service_tier` value based on the processing mode actually used to serve the request. This response value may be different from the value set in the parameter.
      enum:
        - auto
        - default
        - flex
        - scale
        - priority
      nullable: true
      default: auto
    ModelResponseProperties:
      type: object
      properties:
        metadata:
          $ref: '#/components/schemas/Metadata'
        top_logprobs:
          description: |
            An integer between 0 and 20 specifying the number of most likely tokens to
            return at each token position, each with an associated log probability.
          type: integer
          minimum: 0
          maximum: 20
          nullable: true
        temperature:
          type: number
          minimum: 0
          maximum: 2
          default: 1
          example: 1
          nullable: true
          description: >
            What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output
            more random, while lower values like 0.2 will make it more focused and deterministic.

            We generally recommend altering this or `top_p` but not both.
        top_p:
          type: number
          minimum: 0
          maximum: 1
          default: 1
          example: 1
          nullable: true
          description: |
            An alternative to sampling with temperature, called nucleus sampling,
            where the model considers the results of the tokens with top_p probability
            mass. So 0.1 means only the tokens comprising the top 10% probability mass
            are considered.

            We generally recommend altering this or `temperature` but not both.
        user:
          type: string
          example: user-1234
          description: >
            A stable identifier for your end-users. 

            Used to boost cache hit rates by better bucketing similar requests and  to help OpenAI detect and
            prevent abuse. [Learn more](/docs/guides/safety-best-practices#end-user-ids).
        service_tier:
          $ref: '#/components/schemas/ServiceTier'
    CreateModelResponseProperties:
      allOf:
        - $ref: '#/components/schemas/ModelResponseProperties'
        - type: object
          properties:
            top_logprobs:
              description: |
                An integer between 0 and 20 specifying the number of most likely tokens to
                return at each token position, each with an associated log probability.
              type: integer
              minimum: 0
              maximum: 20
    ChatCompletionRequestMessage:
      oneOf:
        - $ref: '#/components/schemas/ChatCompletionRequestDeveloperMessage'
        - $ref: '#/components/schemas/ChatCompletionRequestSystemMessage'
        - $ref: '#/components/schemas/ChatCompletionRequestUserMessage'
        - $ref: '#/components/schemas/ChatCompletionRequestAssistantMessage'
        - $ref: '#/components/schemas/ChatCompletionRequestToolMessage'
        - $ref: '#/components/schemas/ChatCompletionRequestFunctionMessage'
    ChatCompletionRequestFunctionMessage:
      type: object
      title: Function message
      deprecated: true
      properties:
        role:
          type: string
          enum:
            - function
          description: The role of the messages author, in this case `function`.
          x-stainless-const: true
        content:
          nullable: true
          type: string
          description: The contents of the function message.
        name:
          type: string
          description: The name of the function to call.
      required:
        - role
        - content
        - name
    ChatCompletionRequestToolMessage:
      type: object
      title: Tool message
      properties:
        role:
          type: string
          enum:
            - tool
          description: The role of the messages author, in this case `tool`.
          x-stainless-const: true
        content:
          oneOf:
            - type: string
              description: The contents of the tool message.
              title: Text content
            - type: array
              description: >-
                An array of content parts with a defined type. For tool messages, only type `text` is
                supported.
              title: Array of content parts
              items:
                $ref: '#/components/schemas/ChatCompletionRequestToolMessageContentPart'
              minItems: 1
          description: The contents of the tool message.
        tool_call_id:
          type: string
          description: Tool call that this message is responding to.
      required:
        - role
        - content
        - tool_call_id
    ChatCompletionRequestToolMessageContentPart:
      oneOf:
        - $ref: '#/components/schemas/ChatCompletionRequestMessageContentPartText'
    ChatCompletionRequestAssistantMessage:
      type: object
      title: Assistant message
      description: |
        Messages sent by the model in response to user messages.
      properties:
        content:
          nullable: true
          oneOf:
            - type: string
              description: The contents of the assistant message.
              title: Text content
            - type: array
              description: >-
                An array of content parts with a defined type. Can be one or more of type `text`, or exactly
                one of type `refusal`.
              title: Array of content parts
              items:
                $ref: '#/components/schemas/ChatCompletionRequestAssistantMessageContentPart'
              minItems: 1
          description: >
            The contents of the assistant message. Required unless `tool_calls` or `function_call` is
            specified.
        refusal:
          nullable: true
          type: string
          description: The refusal message by the assistant.
        role:
          type: string
          enum:
            - assistant
          description: The role of the messages author, in this case `assistant`.
          x-stainless-const: true
        name:
          type: string
          description: >-
            An optional name for the participant. Provides the model information to differentiate between
            participants of the same role.
        audio:
          type: object
          nullable: true
          description: |
            Data about a previous audio response from the model. 
            [Learn more](/docs/guides/audio).
          required:
            - id
          properties:
            id:
              type: string
              description: |
                Unique identifier for a previous audio response from the model.
        tool_calls:
          $ref: '#/components/schemas/ChatCompletionMessageToolCalls'
        function_call:
          type: object
          deprecated: true
          description: >-
            Deprecated and replaced by `tool_calls`. The name and arguments of a function that should be
            called, as generated by the model.
          nullable: true
          properties:
            arguments:
              type: string
              description: >-
                The arguments to call the function with, as generated by the model in JSON format. Note that
                the model does not always generate valid JSON, and may hallucinate parameters not defined by
                your function schema. Validate the arguments in your code before calling your function.
            name:
              type: string
              description: The name of the function to call.
          required:
            - arguments
            - name
      required:
        - role
    ChatCompletionMessageToolCalls:
      type: array
      description: The tool calls generated by the model, such as function calls.
      items:
        $ref: '#/components/schemas/ChatCompletionMessageToolCall'
    ChatCompletionMessageToolCall:
      type: object
      properties:
        id:
          type: string
          description: The ID of the tool call.
        type:
          type: string
          enum:
            - function
          description: The type of the tool. Currently, only `function` is supported.
          x-stainless-const: true
        function:
          type: object
          description: The function that the model called.
          properties:
            name:
              type: string
              description: The name of the function to call.
            arguments:
              type: string
              description: >-
                The arguments to call the function with, as generated by the model in JSON format. Note that
                the model does not always generate valid JSON, and may hallucinate parameters not defined by
                your function schema. Validate the arguments in your code before calling your function.
          required:
            - name
            - arguments
      required:
        - id
        - type
        - function
    ChatCompletionRequestAssistantMessageContentPart:
      oneOf:
        - $ref: '#/components/schemas/ChatCompletionRequestMessageContentPartText'
        - $ref: '#/components/schemas/ChatCompletionRequestMessageContentPartRefusal'
    ChatCompletionRequestMessageContentPartRefusal:
      type: object
      title: Refusal content part
      properties:
        type:
          type: string
          enum:
            - refusal
          description: The type of the content part.
          x-stainless-const: true
        refusal:
          type: string
          description: The refusal message generated by the model.
      required:
        - type
        - refusal
    ChatCompletionRequestUserMessage:
      type: object
      title: User message
      description: |
        Messages sent by an end user, containing prompts or additional context
        information.
      properties:
        content:
          description: |
            The contents of the user message.
          oneOf:
            - type: string
              description: The text contents of the message.
              title: Text content
            - type: array
              description: >-
                An array of content parts with a defined type. Supported options differ based on the
                [model](/docs/models) being used to generate the response. Can contain text, image, or audio
                inputs.
              title: Array of content parts
              items:
                $ref: '#/components/schemas/ChatCompletionRequestUserMessageContentPart'
              minItems: 1
        role:
          type: string
          enum:
            - user
          description: The role of the messages author, in this case `user`.
          x-stainless-const: true
        name:
          type: string
          description: >-
            An optional name for the participant. Provides the model information to differentiate between
            participants of the same role.
      required:
        - content
        - role
    ChatCompletionRequestUserMessageContentPart:
      oneOf:
        - $ref: '#/components/schemas/ChatCompletionRequestMessageContentPartText'
        - $ref: '#/components/schemas/ChatCompletionRequestMessageContentPartImage'
        - $ref: '#/components/schemas/ChatCompletionRequestMessageContentPartAudio'
        - $ref: '#/components/schemas/ChatCompletionRequestMessageContentPartFile'
    ChatCompletionRequestMessageContentPartFile:
      type: object
      title: File content part
      description: |
        Learn about [file inputs](/docs/guides/text) for text generation.
      properties:
        type:
          type: string
          enum:
            - file
          description: The type of the content part. Always `file`.
          x-stainless-const: true
        file:
          type: object
          properties:
            filename:
              type: string
              description: |
                The name of the file, used when passing the file to the model as a 
                string.
            file_data:
              type: string
              description: |
                The base64 encoded file data, used when passing the file to the model 
                as a string.
            file_id:
              type: string
              description: |
                The ID of an uploaded file to use as input.
      required:
        - type
        - file
    ChatCompletionRequestMessageContentPartAudio:
      type: object
      title: Audio content part
      description: |
        Learn about [audio inputs](/docs/guides/audio).
      properties:
        type:
          type: string
          enum:
            - input_audio
          description: The type of the content part. Always `input_audio`.
          x-stainless-const: true
        input_audio:
          type: object
          properties:
            data:
              type: string
              description: Base64 encoded audio data.
            format:
              type: string
              enum:
                - wav
                - mp3
              description: |
                The format of the encoded audio data. Currently supports "wav" and "mp3".
          required:
            - data
            - format
      required:
        - type
        - input_audio
    ChatCompletionRequestMessageContentPartImage:
      type: object
      title: Image content part
      description: |
        Learn about [image inputs](/docs/guides/vision).
      properties:
        type:
          type: string
          enum:
            - image_url
          description: The type of the content part.
          x-stainless-const: true
        image_url:
          type: object
          properties:
            url:
              type: string
              description: Either a URL of the image or the base64 encoded image data.
              format: uri
            detail:
              type: string
              description: >-
                Specifies the detail level of the image. Learn more in the [Vision
                guide](/docs/guides/vision#low-or-high-fidelity-image-understanding).
              enum:
                - auto
                - low
                - high
              default: auto
          required:
            - url
      required:
        - type
        - image_url
    ChatCompletionRequestSystemMessage:
      type: object
      title: System message
      description: |
        Developer-provided instructions that the model should follow, regardless of
        messages sent by the user. With o1 models and newer, use `developer` messages
        for this purpose instead.
      properties:
        content:
          description: The contents of the system message.
          oneOf:
            - type: string
              description: The contents of the system message.
              title: Text content
            - type: array
              description: >-
                An array of content parts with a defined type. For system messages, only type `text` is
                supported.
              title: Array of content parts
              items:
                $ref: '#/components/schemas/ChatCompletionRequestSystemMessageContentPart'
              minItems: 1
        role:
          type: string
          enum:
            - system
          description: The role of the messages author, in this case `system`.
          x-stainless-const: true
        name:
          type: string
          description: >-
            An optional name for the participant. Provides the model information to differentiate between
            participants of the same role.
      required:
        - content
        - role
    ChatCompletionRequestSystemMessageContentPart:
      oneOf:
        - $ref: '#/components/schemas/ChatCompletionRequestMessageContentPartText'
    ChatCompletionRequestDeveloperMessage:
      type: object
      title: Developer message
      description: |
        Developer-provided instructions that the model should follow, regardless of
        messages sent by the user. With o1 models and newer, `developer` messages
        replace the previous `system` messages.
      properties:
        content:
          description: The contents of the developer message.
          oneOf:
            - type: string
              description: The contents of the developer message.
              title: Text content
            - type: array
              description: >-
                An array of content parts with a defined type. For developer messages, only type `text` is
                supported.
              title: Array of content parts
              items:
                $ref: '#/components/schemas/ChatCompletionRequestMessageContentPartText'
              minItems: 1
        role:
          type: string
          enum:
            - developer
          description: The role of the messages author, in this case `developer`.
          x-stainless-const: true
        name:
          type: string
          description: >-
            An optional name for the participant. Provides the model information to differentiate between
            participants of the same role.
      required:
        - content
        - role
    ChatCompletionRequestMessageContentPartText:
      type: object
      title: Text content part
      description: |
        Learn about [text inputs](/docs/guides/text-generation).
      properties:
        type:
          type: string
          enum:
            - text
          description: The type of the content part.
          x-stainless-const: true
        text:
          type: string
          description: The text content.
      required:
        - type
        - text
    ModelIdsShared:
      example: gpt-4o
      anyOf:
        - type: string
        - type: string
          enum:
            - gpt-4.1
            - gpt-4.1-mini
            - gpt-4.1-nano
            - gpt-4.1-2025-04-14
            - gpt-4.1-mini-2025-04-14
            - gpt-4.1-nano-2025-04-14
            - o4-mini
            - o4-mini-2025-04-16
            - o3
            - o3-2025-04-16
            - o3-mini
            - o3-mini-2025-01-31
            - o1
            - o1-2024-12-17
            - o1-preview
            - o1-preview-2024-09-12
            - o1-mini
            - o1-mini-2024-09-12
            - gpt-4o
            - gpt-4o-2024-11-20
            - gpt-4o-2024-08-06
            - gpt-4o-2024-05-13
            - gpt-4o-audio-preview
            - gpt-4o-audio-preview-2024-10-01
            - gpt-4o-audio-preview-2024-12-17
            - gpt-4o-audio-preview-2025-06-03
            - gpt-4o-mini-audio-preview
            - gpt-4o-mini-audio-preview-2024-12-17
            - gpt-4o-search-preview
            - gpt-4o-mini-search-preview
            - gpt-4o-search-preview-2025-03-11
            - gpt-4o-mini-search-preview-2025-03-11
            - chatgpt-4o-latest
            - codex-mini-latest
            - gpt-4o-mini
            - gpt-4o-mini-2024-07-18
            - gpt-4-turbo
            - gpt-4-turbo-2024-04-09
            - gpt-4-0125-preview
            - gpt-4-turbo-preview
            - gpt-4-1106-preview
            - gpt-4-vision-preview
            - gpt-4
            - gpt-4-0314
            - gpt-4-0613
            - gpt-4-32k
            - gpt-4-32k-0314
            - gpt-4-32k-0613
            - gpt-3.5-turbo
            - gpt-3.5-turbo-16k
            - gpt-3.5-turbo-0301
            - gpt-3.5-turbo-0613
            - gpt-3.5-turbo-1106
            - gpt-3.5-turbo-0125
            - gpt-3.5-turbo-16k-0613
    ResponseModalities:
      type: array
      nullable: true
      description: |
        Output types that you would like the model to generate.
        Most models are capable of generating text, which is the default:

        `["text"]`

        The `gpt-4o-audio-preview` model can also be used to 
        [generate audio](/docs/guides/audio). To request that this model generate 
        both text and audio responses, you can use:

        `["text", "audio"]`
      items:
        type: string
        enum:
          - text
          - audio
    ReasoningEffort:
      type: string
      enum:
        - low
        - medium
        - high
      default: medium
      nullable: true
      description: |
        **o-series models only** 

        Constrains effort on reasoning for 
        [reasoning models](https://platform.openai.com/docs/guides/reasoning).
        Currently supported values are `low`, `medium`, and `high`. Reducing
        reasoning effort can result in faster responses and fewer tokens used
        on reasoning in a response.
    WebSearchLocation:
      type: object
      title: Web search location
      description: Approximate location parameters for the search.
      properties:
        country:
          type: string
          description: |
            The two-letter 
            [ISO country code](https://en.wikipedia.org/wiki/ISO_3166-1) of the user,
            e.g. `US`.
        region:
          type: string
          description: |
            Free text input for the region of the user, e.g. `California`.
        city:
          type: string
          description: |
            Free text input for the city of the user, e.g. `San Francisco`.
        timezone:
          type: string
          description: |
            The [IANA timezone](https://timeapi.io/documentation/iana-timezones) 
            of the user, e.g. `America/Los_Angeles`.
    WebSearchContextSize:
      type: string
      description: |
        High level guidance for the amount of context window space to use for the 
        search. One of `low`, `medium`, or `high`. `medium` is the default.
      enum:
        - low
        - medium
        - high
      default: medium
    ResponseFormatText:
      type: object
      title: Text
      description: |
        Default response format. Used to generate text responses.
      properties:
        type:
          type: string
          description: The type of response format being defined. Always `text`.
          enum:
            - text
          x-stainless-const: true
      required:
        - type
    ResponseFormatJsonSchema:
      type: object
      title: JSON schema
      description: |
        JSON Schema response format. Used to generate structured JSON responses.
        Learn more about [Structured Outputs](/docs/guides/structured-outputs).
      properties:
        type:
          type: string
          description: The type of response format being defined. Always `json_schema`.
          enum:
            - json_schema
          x-stainless-const: true
        json_schema:
          type: object
          title: JSON schema
          description: |
            Structured Outputs configuration options, including a JSON Schema.
          properties:
            description:
              type: string
              description: |
                A description of what the response format is for, used by the model to
                determine how to respond in the format.
            name:
              type: string
              description: |
                The name of the response format. Must be a-z, A-Z, 0-9, or contain
                underscores and dashes, with a maximum length of 64.
            schema:
              $ref: '#/components/schemas/ResponseFormatJsonSchemaSchema'
            strict:
              type: boolean
              nullable: true
              default: false
              description: |
                Whether to enable strict schema adherence when generating the output.
                If set to true, the model will always follow the exact schema defined
                in the `schema` field. Only a subset of JSON Schema is supported when
                `strict` is `true`. To learn more, read the [Structured Outputs
                guide](/docs/guides/structured-outputs).
          required:
            - name
      required:
        - type
        - json_schema
    ResponseFormatJsonSchemaSchema:
      type: object
      title: JSON schema
      description: |
        The schema for the response format, described as a JSON Schema object.
        Learn how to build JSON schemas [here](https://json-schema.org/).
      additionalProperties: true
    ResponseFormatJsonObject:
      type: object
      title: JSON object
      description: |
        JSON object response format. An older method of generating JSON responses.
        Using `json_schema` is recommended for models that support it. Note that the
        model will not generate JSON without a system or user message instructing it
        to do so.
      properties:
        type:
          type: string
          description: The type of response format being defined. Always `json_object`.
          enum:
            - json_object
          x-stainless-const: true
      required:
        - type
    VoiceIdsShared:
      example: ash
      anyOf:
        - type: string
        - type: string
          enum:
            - alloy
            - ash
            - ballad
            - coral
            - echo
            - fable
            - onyx
            - nova
            - sage
            - shimmer
            - verse
    StopConfiguration:
      description: |
        Not supported with latest reasoning models `o3` and `o4-mini`.

        Up to 4 sequences where the API will stop generating further tokens. The
        returned text will not contain the stop sequence.
      default: null
      nullable: true
      oneOf:
        - type: string
          default: <|endoftext|>
          example: |+

          nullable: true
        - type: array
          minItems: 1
          maxItems: 4
          items:
            type: string
            example: '["\n"]'
    PredictionContent:
      type: object
      title: Static Content
      description: |
        Static predicted output content, such as the content of a text file that is
        being regenerated.
      required:
        - type
        - content
      properties:
        type:
          type: string
          enum:
            - content
          description: |
            The type of the predicted content you want to provide. This type is
            currently always `content`.
          x-stainless-const: true
        content:
          description: |
            The content that should be matched when generating a model response.
            If generated tokens would match this content, the entire model response
            can be returned much more quickly.
          oneOf:
            - type: string
              title: Text content
              description: |
                The content used for a Predicted Output. This is often the
                text of a file you are regenerating with minor changes.
            - type: array
              description: >-
                An array of content parts with a defined type. Supported options differ based on the
                [model](/docs/models) being used to generate the response. Can contain text inputs.
              title: Array of content parts
              items:
                $ref: '#/components/schemas/ChatCompletionRequestMessageContentPartText'
              minItems: 1
    ChatCompletionStreamOptions:
      description: |
        Options for streaming response. Only set this when you set `stream: true`.
      type: object
      nullable: true
      default: null
      properties:
        include_usage:
          type: boolean
          description: |
            If set, an additional chunk will be streamed before the `data: [DONE]`
            message. The `usage` field on this chunk shows the token usage statistics
            for the entire request, and the `choices` field will always be an empty
            array. 

            All other chunks will also include a `usage` field, but with a null
            value. **NOTE:** If the stream is interrupted, you may not receive the
            final usage chunk which contains the total token usage for the request.
    ChatCompletionTool:
      type: object
      properties:
        type:
          type: string
          enum:
            - function
          description: The type of the tool. Currently, only `function` is supported.
          x-stainless-const: true
        function:
          $ref: '#/components/schemas/FunctionObject'
      required:
        - type
        - function
    FunctionObject:
      type: object
      properties:
        description:
          type: string
          description: >-
            A description of what the function does, used by the model to choose when and how to call the
            function.
        name:
          type: string
          description: >-
            The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes,
            with a maximum length of 64.
        parameters:
          $ref: '#/components/schemas/FunctionParameters'
        strict:
          type: boolean
          nullable: true
          default: false
          description: >-
            Whether to enable strict schema adherence when generating the function call. If set to true, the
            model will follow the exact schema defined in the `parameters` field. Only a subset of JSON Schema
            is supported when `strict` is `true`. Learn more about Structured Outputs in the [function calling
            guide](docs/guides/function-calling).
      required:
        - name
    FunctionParameters:
      type: object
      description: >-
        The parameters the functions accepts, described as a JSON Schema object. See the
        [guide](/docs/guides/function-calling) for examples, and the [JSON Schema
        reference](https://json-schema.org/understanding-json-schema/) for documentation about the format. 


        Omitting `parameters` defines a function with an empty parameter list.
      additionalProperties: true
    ChatCompletionToolChoiceOption:
      description: >
        Controls which (if any) tool is called by the model.

        `none` means the model will not call any tool and instead generates a message.

        `auto` means the model can pick between generating a message or calling one or more tools.

        `required` means the model must call one or more tools.

        Specifying a particular tool via `{"type": "function", "function": {"name": "my_function"}}` forces
        the model to call that tool.


        `none` is the default when no tools are present. `auto` is the default if tools are present.
      oneOf:
        - type: string
          description: >
            `none` means the model will not call any tool and instead generates a message. `auto` means the
            model can pick between generating a message or calling one or more tools. `required` means the
            model must call one or more tools.
          enum:
            - none
            - auto
            - required
        - $ref: '#/components/schemas/ChatCompletionNamedToolChoice'
    ChatCompletionNamedToolChoice:
      type: object
      description: Specifies a tool the model should use. Use to force the model to call a specific function.
      properties:
        type:
          type: string
          enum:
            - function
          description: The type of the tool. Currently, only `function` is supported.
          x-stainless-const: true
        function:
          type: object
          properties:
            name:
              type: string
              description: The name of the function to call.
          required:
            - name
      required:
        - type
        - function
    ChatCompletionFunctionCallOption:
      type: object
      description: |
        Specifying a particular function via `{"name": "my_function"}` forces the model to call that function.
      properties:
        name:
          type: string
          description: The name of the function to call.
      required:
        - name
    ChatCompletionFunctions:
      type: object
      deprecated: true
      properties:
        description:
          type: string
          description: >-
            A description of what the function does, used by the model to choose when and how to call the
            function.
        name:
          type: string
          description: >-
            The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes,
            with a maximum length of 64.
        parameters:
          $ref: '#/components/schemas/FunctionParameters'
      required:
        - name
    ParallelToolCalls:
      description: >-
        Whether to enable [parallel function
        calling](/docs/guides/function-calling#configuring-parallel-function-calling) during tool use.
      type: boolean
      default: true
    CreateChatCompletionRequest:
      allOf:
        - $ref: '#/components/schemas/CreateModelResponseProperties'
        - type: object
          properties:
            messages:
              description: |
                A list of messages comprising the conversation so far. Depending on the
                [model](/docs/models) you use, different message types (modalities) are
                supported, like [text](/docs/guides/text-generation),
                [images](/docs/guides/vision), and [audio](/docs/guides/audio).
              type: array
              minItems: 1
              items:
                $ref: '#/components/schemas/ChatCompletionRequestMessage'
            model:
              description: |
                Model ID used to generate the response, like `gpt-4o` or `o3`. OpenAI
                offers a wide range of models with different capabilities, performance
                characteristics, and price points. Refer to the [model guide](/docs/models)
                to browse and compare available models.
              $ref: '#/components/schemas/ModelIdsShared'
            modalities:
              $ref: '#/components/schemas/ResponseModalities'
            reasoning_effort:
              $ref: '#/components/schemas/ReasoningEffort'
            max_completion_tokens:
              description: >
                An upper bound for the number of tokens that can be generated for a completion, including
                visible output tokens and [reasoning tokens](/docs/guides/reasoning).
              type: integer
              nullable: true
            frequency_penalty:
              type: number
              default: 0
              minimum: -2
              maximum: 2
              nullable: true
              description: |
                Number between -2.0 and 2.0. Positive values penalize new tokens based on
                their existing frequency in the text so far, decreasing the model's
                likelihood to repeat the same line verbatim.
            presence_penalty:
              type: number
              default: 0
              minimum: -2
              maximum: 2
              nullable: true
              description: |
                Number between -2.0 and 2.0. Positive values penalize new tokens based on
                whether they appear in the text so far, increasing the model's likelihood
                to talk about new topics.
            web_search_options:
              type: object
              title: Web search
              description: |
                This tool searches the web for relevant results to use in a response.
                Learn more about the [web search tool](/docs/guides/tools-web-search?api-mode=chat).
              properties:
                user_location:
                  type: object
                  nullable: true
                  required:
                    - type
                    - approximate
                  description: |
                    Approximate location parameters for the search.
                  properties:
                    type:
                      type: string
                      description: |
                        The type of location approximation. Always `approximate`.
                      enum:
                        - approximate
                      x-stainless-const: true
                    approximate:
                      $ref: '#/components/schemas/WebSearchLocation'
                search_context_size:
                  $ref: '#/components/schemas/WebSearchContextSize'
            top_logprobs:
              description: |
                An integer between 0 and 20 specifying the number of most likely tokens to
                return at each token position, each with an associated log probability.
                `logprobs` must be set to `true` if this parameter is used.
              type: integer
              minimum: 0
              maximum: 20
              nullable: true
            response_format:
              description: |
                An object specifying the format that the model must output.

                Setting to `{ "type": "json_schema", "json_schema": {...} }` enables
                Structured Outputs which ensures the model will match your supplied JSON
                schema. Learn more in the [Structured Outputs
                guide](/docs/guides/structured-outputs).

                Setting to `{ "type": "json_object" }` enables the older JSON mode, which
                ensures the message the model generates is valid JSON. Using `json_schema`
                is preferred for models that support it.
              oneOf:
                - $ref: '#/components/schemas/ResponseFormatText'
                - $ref: '#/components/schemas/ResponseFormatJsonSchema'
                - $ref: '#/components/schemas/ResponseFormatJsonObject'
            audio:
              type: object
              nullable: true
              description: |
                Parameters for audio output. Required when audio output is requested with
                `modalities: ["audio"]`. [Learn more](/docs/guides/audio).
              required:
                - voice
                - format
              properties:
                voice:
                  $ref: '#/components/schemas/VoiceIdsShared'
                  description: |
                    The voice the model uses to respond. Supported voices are 
                    `alloy`, `ash`, `ballad`, `coral`, `echo`, `fable`, `nova`, `onyx`, `sage`, and `shimmer`.
                format:
                  type: string
                  enum:
                    - wav
                    - aac
                    - mp3
                    - flac
                    - opus
                    - pcm16
                  description: |
                    Specifies the output audio format. Must be one of `wav`, `mp3`, `flac`,
                    `opus`, or `pcm16`.
            store:
              type: boolean
              default: false
              nullable: true
              description: |
                Whether or not to store the output of this chat completion request for 
                use in our [model distillation](/docs/guides/distillation) or
                [evals](/docs/guides/evals) products. 

                Supports text and image inputs. Note: image inputs over 10MB will be dropped.
            stream:
              description: >
                If set to true, the model response data will be streamed to the client

                as it is generated using [server-sent
                events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format).

                See the [Streaming section below](/docs/api-reference/chat/streaming)

                for more information, along with the [streaming responses](/docs/guides/streaming-responses)

                guide for more information on how to handle the streaming events.
              type: boolean
              nullable: true
              default: false
            stop:
              $ref: '#/components/schemas/StopConfiguration'
            logit_bias:
              type: object
              x-oaiTypeLabel: map
              default: null
              nullable: true
              additionalProperties:
                type: integer
              description: |
                Modify the likelihood of specified tokens appearing in the completion.

                Accepts a JSON object that maps tokens (specified by their token ID in the
                tokenizer) to an associated bias value from -100 to 100. Mathematically,
                the bias is added to the logits generated by the model prior to sampling.
                The exact effect will vary per model, but values between -1 and 1 should
                decrease or increase likelihood of selection; values like -100 or 100
                should result in a ban or exclusive selection of the relevant token.
            logprobs:
              description: |
                Whether to return log probabilities of the output tokens or not. If true,
                returns the log probabilities of each output token returned in the
                `content` of `message`.
              type: boolean
              default: false
              nullable: true
            max_tokens:
              description: |
                The maximum number of [tokens](/tokenizer) that can be generated in the
                chat completion. This value can be used to control
                [costs](https://openai.com/api/pricing/) for text generated via API.

                This value is now deprecated in favor of `max_completion_tokens`, and is
                not compatible with [o-series models](/docs/guides/reasoning).
              type: integer
              nullable: true
              deprecated: true
            'n':
              type: integer
              minimum: 1
              maximum: 128
              default: 1
              example: 1
              nullable: true
              description: >-
                How many chat completion choices to generate for each input message. Note that you will be
                charged based on the number of generated tokens across all of the choices. Keep `n` as `1` to
                minimize costs.
            prediction:
              nullable: true
              description: |
                Configuration for a [Predicted Output](/docs/guides/predicted-outputs),
                which can greatly improve response times when large parts of the model
                response are known ahead of time. This is most common when you are
                regenerating a file with only minor changes to most of the content.
              oneOf:
                - $ref: '#/components/schemas/PredictionContent'
            seed:
              type: integer
              minimum: -9223372036854776000
              maximum: 9223372036854776000
              nullable: true
              description: >
                This feature is in Beta.

                If specified, our system will make a best effort to sample deterministically, such that
                repeated requests with the same `seed` and parameters should return the same result.

                Determinism is not guaranteed, and you should refer to the `system_fingerprint` response
                parameter to monitor changes in the backend.
              x-oaiMeta:
                beta: true
            stream_options:
              $ref: '#/components/schemas/ChatCompletionStreamOptions'
            tools:
              type: array
              description: >
                A list of tools the model may call. Currently, only functions are supported as a tool. Use
                this to provide a list of functions the model may generate JSON inputs for. A max of 128
                functions are supported.
              items:
                $ref: '#/components/schemas/ChatCompletionTool'
            tool_choice:
              $ref: '#/components/schemas/ChatCompletionToolChoiceOption'
            parallel_tool_calls:
              $ref: '#/components/schemas/ParallelToolCalls'
            function_call:
              deprecated: true
              description: |
                Deprecated in favor of `tool_choice`.

                Controls which (if any) function is called by the model.

                `none` means the model will not call a function and instead generates a
                message.

                `auto` means the model can pick between generating a message or calling a
                function.

                Specifying a particular function via `{"name": "my_function"}` forces the
                model to call that function.

                `none` is the default when no functions are present. `auto` is the default
                if functions are present.
              oneOf:
                - type: string
                  description: >
                    `none` means the model will not call a function and instead generates a message. `auto`
                    means the model can pick between generating a message or calling a function.
                  enum:
                    - none
                    - auto
                - $ref: '#/components/schemas/ChatCompletionFunctionCallOption'
            functions:
              deprecated: true
              description: |
                Deprecated in favor of `tools`.

                A list of functions the model may generate JSON inputs for.
              type: array
              minItems: 1
              maxItems: 128
              items:
                $ref: '#/components/schemas/ChatCompletionFunctions'
          required:
            - model
            - messages
    ChatCompletionResponseMessage:
      type: object
      description: A chat completion message generated by the model.
      properties:
        content:
          type: string
          description: The contents of the message.
          nullable: true
        refusal:
          type: string
          description: The refusal message generated by the model.
          nullable: true
        tool_calls:
          $ref: '#/components/schemas/ChatCompletionMessageToolCalls'
        annotations:
          type: array
          description: |
            Annotations for the message, when applicable, as when using the
            [web search tool](/docs/guides/tools-web-search?api-mode=chat).
          items:
            type: object
            description: |
              A URL citation when using web search.
            required:
              - type
              - url_citation
            properties:
              type:
                type: string
                description: The type of the URL citation. Always `url_citation`.
                enum:
                  - url_citation
                x-stainless-const: true
              url_citation:
                type: object
                description: A URL citation when using web search.
                required:
                  - end_index
                  - start_index
                  - url
                  - title
                properties:
                  end_index:
                    type: integer
                    description: The index of the last character of the URL citation in the message.
                  start_index:
                    type: integer
                    description: The index of the first character of the URL citation in the message.
                  url:
                    type: string
                    description: The URL of the web resource.
                  title:
                    type: string
                    description: The title of the web resource.
        role:
          type: string
          enum:
            - assistant
          description: The role of the author of this message.
          x-stainless-const: true
        function_call:
          type: object
          deprecated: true
          description: >-
            Deprecated and replaced by `tool_calls`. The name and arguments of a function that should be
            called, as generated by the model.
          properties:
            arguments:
              type: string
              description: >-
                The arguments to call the function with, as generated by the model in JSON format. Note that
                the model does not always generate valid JSON, and may hallucinate parameters not defined by
                your function schema. Validate the arguments in your code before calling your function.
            name:
              type: string
              description: The name of the function to call.
          required:
            - name
            - arguments
        audio:
          type: object
          nullable: true
          description: |
            If the audio output modality is requested, this object contains data
            about the audio response from the model. [Learn more](/docs/guides/audio).
          required:
            - id
            - expires_at
            - data
            - transcript
          properties:
            id:
              type: string
              description: Unique identifier for this audio response.
            expires_at:
              type: integer
              description: |
                The Unix timestamp (in seconds) for when this audio response will
                no longer be accessible on the server for use in multi-turn
                conversations.
            data:
              type: string
              description: |
                Base64 encoded audio bytes generated by the model, in the format
                specified in the request.
            transcript:
              type: string
              description: Transcript of the audio generated by the model.
      required:
        - role
        - content
        - refusal
    ChatCompletionTokenLogprob:
      type: object
      properties:
        token:
          description: The token.
          type: string
        logprob:
          description: >-
            The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the
            value `-9999.0` is used to signify that the token is very unlikely.
          type: number
        bytes:
          description: >-
            A list of integers representing the UTF-8 bytes representation of the token. Useful in instances
            where characters are represented by multiple tokens and their byte representations must be
            combined to generate the correct text representation. Can be `null` if there is no bytes
            representation for the token.
          type: array
          items:
            type: integer
          nullable: true
        top_logprobs:
          description: >-
            List of the most likely tokens and their log probability, at this token position. In rare cases,
            there may be fewer than the number of requested `top_logprobs` returned.
          type: array
          items:
            type: object
            properties:
              token:
                description: The token.
                type: string
              logprob:
                description: >-
                  The log probability of this token, if it is within the top 20 most likely tokens. Otherwise,
                  the value `-9999.0` is used to signify that the token is very unlikely.
                type: number
              bytes:
                description: >-
                  A list of integers representing the UTF-8 bytes representation of the token. Useful in
                  instances where characters are represented by multiple tokens and their byte representations
                  must be combined to generate the correct text representation. Can be `null` if there is no
                  bytes representation for the token.
                type: array
                items:
                  type: integer
                nullable: true
            required:
              - token
              - logprob
              - bytes
      required:
        - token
        - logprob
        - bytes
        - top_logprobs
    CompletionUsage:
      type: object
      description: Usage statistics for the completion request.
      properties:
        completion_tokens:
          type: integer
          default: 0
          description: Number of tokens in the generated completion.
        prompt_tokens:
          type: integer
          default: 0
          description: Number of tokens in the prompt.
        total_tokens:
          type: integer
          default: 0
          description: Total number of tokens used in the request (prompt + completion).
        completion_tokens_details:
          type: object
          description: Breakdown of tokens used in a completion.
          properties:
            accepted_prediction_tokens:
              type: integer
              default: 0
              description: |
                When using Predicted Outputs, the number of tokens in the
                prediction that appeared in the completion.
            audio_tokens:
              type: integer
              default: 0
              description: Audio input tokens generated by the model.
            reasoning_tokens:
              type: integer
              default: 0
              description: Tokens generated by the model for reasoning.
            rejected_prediction_tokens:
              type: integer
              default: 0
              description: |
                When using Predicted Outputs, the number of tokens in the
                prediction that did not appear in the completion. However, like
                reasoning tokens, these tokens are still counted in the total
                completion tokens for purposes of billing, output, and context window
                limits.
        prompt_tokens_details:
          type: object
          description: Breakdown of tokens used in the prompt.
          properties:
            audio_tokens:
              type: integer
              default: 0
              description: Audio input tokens present in the prompt.
            cached_tokens:
              type: integer
              default: 0
              description: Cached tokens present in the prompt.
      required:
        - prompt_tokens
        - completion_tokens
        - total_tokens
    CreateChatCompletionResponse:
      type: object
      description: Represents a chat completion response returned by model, based on the provided input.
      properties:
        id:
          type: string
          description: A unique identifier for the chat completion.
        choices:
          type: array
          description: A list of chat completion choices. Can be more than one if `n` is greater than 1.
          items:
            type: object
            required:
              - finish_reason
              - index
              - message
              - logprobs
            properties:
              finish_reason:
                type: string
                description: >
                  The reason the model stopped generating tokens. This will be `stop` if the model hit a
                  natural stop point or a provided stop sequence,

                  `length` if the maximum number of tokens specified in the request was reached,

                  `content_filter` if content was omitted due to a flag from our content filters,

                  `tool_calls` if the model called a tool, or `function_call` (deprecated) if the model called
                  a function.
                enum:
                  - stop
                  - length
                  - tool_calls
                  - content_filter
                  - function_call
              index:
                type: integer
                description: The index of the choice in the list of choices.
              message:
                $ref: '#/components/schemas/ChatCompletionResponseMessage'
              logprobs:
                description: Log probability information for the choice.
                type: object
                nullable: true
                properties:
                  content:
                    description: A list of message content tokens with log probability information.
                    type: array
                    items:
                      $ref: '#/components/schemas/ChatCompletionTokenLogprob'
                    nullable: true
                  refusal:
                    description: A list of message refusal tokens with log probability information.
                    type: array
                    items:
                      $ref: '#/components/schemas/ChatCompletionTokenLogprob'
                    nullable: true
                required:
                  - content
                  - refusal
        created:
          type: integer
          description: The Unix timestamp (in seconds) of when the chat completion was created.
        model:
          type: string
          description: The model used for the chat completion.
        service_tier:
          $ref: '#/components/schemas/ServiceTier'
        system_fingerprint:
          type: string
          description: >
            This fingerprint represents the backend configuration that the model runs with.


            Can be used in conjunction with the `seed` request parameter to understand when backend changes
            have been made that might impact determinism.
        object:
          type: string
          description: The object type, which is always `chat.completion`.
          enum:
            - chat.completion
          x-stainless-const: true
        usage:
          $ref: '#/components/schemas/CompletionUsage'
      required:
        - choices
        - created
        - id
        - model
        - object
      x-oaiMeta:
        name: The chat completion object
        group: chat
        example: |
          {
            "id": "chatcmpl-B9MHDbslfkBeAs8l4bebGdFOJ6PeG",
            "object": "chat.completion",
            "created": 1741570283,
            "model": "gpt-4o-2024-08-06",
            "choices": [
              {
                "index": 0,
                "message": {
                  "role": "assistant",
                  "content": "The image shows a wooden boardwalk path running through a lush green field or meadow. The sky is bright blue with some scattered clouds, giving the scene a serene and peaceful atmosphere. Trees and shrubs are visible in the background.",
                  "refusal": null,
                  "annotations": []
                },
                "logprobs": null,
                "finish_reason": "stop"
              }
            ],
            "usage": {
              "prompt_tokens": 1117,
              "completion_tokens": 46,
              "total_tokens": 1163,
              "prompt_tokens_details": {
                "cached_tokens": 0,
                "audio_tokens": 0
              },
              "completion_tokens_details": {
                "reasoning_tokens": 0,
                "audio_tokens": 0,
                "accepted_prediction_tokens": 0,
                "rejected_prediction_tokens": 0
              }
            },
            "service_tier": "default",
            "system_fingerprint": "fp_fc9f1d7035"
          }
    ChatCompletionMessageToolCallChunk:
      type: object
      properties:
        index:
          type: integer
        id:
          type: string
          description: The ID of the tool call.
        type:
          type: string
          enum:
            - function
          description: The type of the tool. Currently, only `function` is supported.
          x-stainless-const: true
        function:
          type: object
          properties:
            name:
              type: string
              description: The name of the function to call.
            arguments:
              type: string
              description: >-
                The arguments to call the function with, as generated by the model in JSON format. Note that
                the model does not always generate valid JSON, and may hallucinate parameters not defined by
                your function schema. Validate the arguments in your code before calling your function.
      required:
        - index
    ChatCompletionStreamResponseDelta:
      type: object
      description: A chat completion delta generated by streamed model responses.
      properties:
        content:
          type: string
          description: The contents of the chunk message.
          nullable: true
        function_call:
          deprecated: true
          type: object
          description: >-
            Deprecated and replaced by `tool_calls`. The name and arguments of a function that should be
            called, as generated by the model.
          properties:
            arguments:
              type: string
              description: >-
                The arguments to call the function with, as generated by the model in JSON format. Note that
                the model does not always generate valid JSON, and may hallucinate parameters not defined by
                your function schema. Validate the arguments in your code before calling your function.
            name:
              type: string
              description: The name of the function to call.
        tool_calls:
          type: array
          items:
            $ref: '#/components/schemas/ChatCompletionMessageToolCallChunk'
        role:
          type: string
          enum:
            - developer
            - system
            - user
            - assistant
            - tool
          description: The role of the author of this message.
        refusal:
          type: string
          description: The refusal message generated by the model.
          nullable: true
    CreateChatCompletionStreamResponse:
      type: object
      description: |
        Represents a streamed chunk of a chat completion response returned
        by the model, based on the provided input. 
        [Learn more](/docs/guides/streaming-responses).
      properties:
        id:
          type: string
          description: A unique identifier for the chat completion. Each chunk has the same ID.
        choices:
          type: array
          description: >
            A list of chat completion choices. Can contain more than one elements if `n` is greater than 1.
            Can also be empty for the

            last chunk if you set `stream_options: {"include_usage": true}`.
          items:
            type: object
            required:
              - delta
              - finish_reason
              - index
            properties:
              delta:
                $ref: '#/components/schemas/ChatCompletionStreamResponseDelta'
              logprobs:
                description: Log probability information for the choice.
                type: object
                nullable: true
                properties:
                  content:
                    description: A list of message content tokens with log probability information.
                    type: array
                    items:
                      $ref: '#/components/schemas/ChatCompletionTokenLogprob'
                    nullable: true
                  refusal:
                    description: A list of message refusal tokens with log probability information.
                    type: array
                    items:
                      $ref: '#/components/schemas/ChatCompletionTokenLogprob'
                    nullable: true
                required:
                  - content
                  - refusal
              finish_reason:
                type: string
                description: >
                  The reason the model stopped generating tokens. This will be `stop` if the model hit a
                  natural stop point or a provided stop sequence,

                  `length` if the maximum number of tokens specified in the request was reached,

                  `content_filter` if content was omitted due to a flag from our content filters,

                  `tool_calls` if the model called a tool, or `function_call` (deprecated) if the model called
                  a function.
                enum:
                  - stop
                  - length
                  - tool_calls
                  - content_filter
                  - function_call
                nullable: true
              index:
                type: integer
                description: The index of the choice in the list of choices.
        created:
          type: integer
          description: >-
            The Unix timestamp (in seconds) of when the chat completion was created. Each chunk has the same
            timestamp.
        model:
          type: string
          description: The model to generate the completion.
        service_tier:
          $ref: '#/components/schemas/ServiceTier'
        system_fingerprint:
          type: string
          description: >
            This fingerprint represents the backend configuration that the model runs with.

            Can be used in conjunction with the `seed` request parameter to understand when backend changes
            have been made that might impact determinism.
        object:
          type: string
          description: The object type, which is always `chat.completion.chunk`.
          enum:
            - chat.completion.chunk
          x-stainless-const: true
        usage:
          $ref: '#/components/schemas/CompletionUsage'
          nullable: true
          description: |
            An optional field that will only be present when you set
            `stream_options: {"include_usage": true}` in your request. When present, it
            contains a null value **except for the last chunk** which contains the
            token usage statistics for the entire request.

            **NOTE:** If the stream is interrupted or cancelled, you may not
            receive the final usage chunk which contains the total token usage for
            the request.
      required:
        - choices
        - created
        - id
        - model
        - object
      x-oaiMeta:
        name: The chat completion chunk object
        group: chat
        example: >
          {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1694268190,"model":"gpt-4o-mini",
          "system_fingerprint": "fp_44709d6fcb",
          "choices":[{"index":0,"delta":{"role":"assistant","content":""},"logprobs":null,"finish_reason":null}]}


          {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1694268190,"model":"gpt-4o-mini",
          "system_fingerprint": "fp_44709d6fcb",
          "choices":[{"index":0,"delta":{"content":"Hello"},"logprobs":null,"finish_reason":null}]}


          ....


          {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1694268190,"model":"gpt-4o-mini",
          "system_fingerprint": "fp_44709d6fcb",
          "choices":[{"index":0,"delta":{},"logprobs":null,"finish_reason":"stop"}]}
    CreateCompletionRequest:
      type: object
      properties:
        model:
          description: >
            ID of the model to use. You can use the [List models](/docs/api-reference/models/list) API to see
            all of your available models, or see our [Model overview](/docs/models) for descriptions of them.
          anyOf:
            - type: string
            - type: string
              enum:
                - gpt-3.5-turbo-instruct
                - davinci-002
                - babbage-002
          x-oaiTypeLabel: string
        prompt:
          description: >
            The prompt(s) to generate completions for, encoded as a string, array of strings, array of tokens,
            or array of token arrays.


            Note that <|endoftext|> is the document separator that the model sees during training, so if a
            prompt is not specified the model will generate as if from the beginning of a new document.
          default: <|endoftext|>
          nullable: true
          oneOf:
            - type: string
              default: ''
              example: This is a test.
            - type: array
              items:
                type: string
                default: ''
                example: This is a test.
            - type: array
              minItems: 1
              items:
                type: integer
              example: '[1212, 318, 257, 1332, 13]'
            - type: array
              minItems: 1
              items:
                type: array
                minItems: 1
                items:
                  type: integer
              example: '[[1212, 318, 257, 1332, 13]]'
        best_of:
          type: integer
          default: 1
          minimum: 0
          maximum: 20
          nullable: true
          description: >
            Generates `best_of` completions server-side and returns the "best" (the one with the highest log
            probability per token). Results cannot be streamed.


            When used with `n`, `best_of` controls the number of candidate completions and `n` specifies how
            many to return  `best_of` must be greater than `n`.


            **Note:** Because this parameter generates many completions, it can quickly consume your token
            quota. Use carefully and ensure that you have reasonable settings for `max_tokens` and `stop`.
        echo:
          type: boolean
          default: false
          nullable: true
          description: |
            Echo back the prompt in addition to the completion
        frequency_penalty:
          type: number
          default: 0
          minimum: -2
          maximum: 2
          nullable: true
          description: >
            Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency
            in the text so far, decreasing the model's likelihood to repeat the same line verbatim.


            [See more information about frequency and presence penalties.](/docs/guides/text-generation)
        logit_bias:
          type: object
          x-oaiTypeLabel: map
          default: null
          nullable: true
          additionalProperties:
            type: integer
          description: >
            Modify the likelihood of specified tokens appearing in the completion.


            Accepts a JSON object that maps tokens (specified by their token ID in the GPT tokenizer) to an
            associated bias value from -100 to 100. You can use this [tokenizer tool](/tokenizer?view=bpe) to
            convert text to token IDs. Mathematically, the bias is added to the logits generated by the model
            prior to sampling. The exact effect will vary per model, but values between -1 and 1 should
            decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or
            exclusive selection of the relevant token.


            As an example, you can pass `{"50256": -100}` to prevent the <|endoftext|> token from being
            generated.
        logprobs:
          type: integer
          minimum: 0
          maximum: 5
          default: null
          nullable: true
          description: >
            Include the log probabilities on the `logprobs` most likely output tokens, as well the chosen
            tokens. For example, if `logprobs` is 5, the API will return a list of the 5 most likely tokens.
            The API will always return the `logprob` of the sampled token, so there may be up to `logprobs+1`
            elements in the response.


            The maximum value for `logprobs` is 5.
        max_tokens:
          type: integer
          minimum: 0
          default: 16
          example: 16
          nullable: true
          description: >
            The maximum number of [tokens](/tokenizer) that can be generated in the completion.


            The token count of your prompt plus `max_tokens` cannot exceed the model's context length.
            [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken) for
            counting tokens.
        'n':
          type: integer
          minimum: 1
          maximum: 128
          default: 1
          example: 1
          nullable: true
          description: >
            How many completions to generate for each prompt.


            **Note:** Because this parameter generates many completions, it can quickly consume your token
            quota. Use carefully and ensure that you have reasonable settings for `max_tokens` and `stop`.
        presence_penalty:
          type: number
          default: 0
          minimum: -2
          maximum: 2
          nullable: true
          description: >
            Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in
            the text so far, increasing the model's likelihood to talk about new topics.


            [See more information about frequency and presence penalties.](/docs/guides/text-generation)
        seed:
          type: integer
          format: int64
          nullable: true
          description: >
            If specified, our system will make a best effort to sample deterministically, such that repeated
            requests with the same `seed` and parameters should return the same result.


            Determinism is not guaranteed, and you should refer to the `system_fingerprint` response parameter
            to monitor changes in the backend.
        stop:
          $ref: '#/components/schemas/StopConfiguration'
        stream:
          description: >
            Whether to stream back partial progress. If set, tokens will be sent as data-only [server-sent
            events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)
            as they become available, with the stream terminated by a `data: [DONE]` message. [Example Python
            code](https://cookbook.openai.com/examples/how_to_stream_completions).
          type: boolean
          nullable: true
          default: false
        stream_options:
          $ref: '#/components/schemas/ChatCompletionStreamOptions'
        suffix:
          description: |
            The suffix that comes after a completion of inserted text.

            This parameter is only supported for `gpt-3.5-turbo-instruct`.
          default: null
          nullable: true
          type: string
          example: test.
        temperature:
          type: number
          minimum: 0
          maximum: 2
          default: 1
          example: 1
          nullable: true
          description: >
            What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output
            more random, while lower values like 0.2 will make it more focused and deterministic.


            We generally recommend altering this or `top_p` but not both.
        top_p:
          type: number
          minimum: 0
          maximum: 1
          default: 1
          example: 1
          nullable: true
          description: >
            An alternative to sampling with temperature, called nucleus sampling, where the model considers
            the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the
            top 10% probability mass are considered.


            We generally recommend altering this or `temperature` but not both.
        user:
          type: string
          example: user-1234
          description: >
            A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse.
            [Learn more](/docs/guides/safety-best-practices#end-user-ids).
      required:
        - model
        - prompt
    CreateCompletionResponse:
      type: object
      description: >
        Represents a completion response from the API. Note: both the streamed and non-streamed response
        objects share the same shape (unlike the chat endpoint).
      properties:
        id:
          type: string
          description: A unique identifier for the completion.
        choices:
          type: array
          description: The list of completion choices the model generated for the input prompt.
          items:
            type: object
            required:
              - finish_reason
              - index
              - logprobs
              - text
            properties:
              finish_reason:
                type: string
                description: >
                  The reason the model stopped generating tokens. This will be `stop` if the model hit a
                  natural stop point or a provided stop sequence,

                  `length` if the maximum number of tokens specified in the request was reached,

                  or `content_filter` if content was omitted due to a flag from our content filters.
                enum:
                  - stop
                  - length
                  - content_filter
              index:
                type: integer
              logprobs:
                type: object
                nullable: true
                properties:
                  text_offset:
                    type: array
                    items:
                      type: integer
                  token_logprobs:
                    type: array
                    items:
                      type: number
                  tokens:
                    type: array
                    items:
                      type: string
                  top_logprobs:
                    type: array
                    items:
                      type: object
                      additionalProperties:
                        type: number
              text:
                type: string
        created:
          type: integer
          description: The Unix timestamp (in seconds) of when the completion was created.
        model:
          type: string
          description: The model used for completion.
        system_fingerprint:
          type: string
          description: >
            This fingerprint represents the backend configuration that the model runs with.


            Can be used in conjunction with the `seed` request parameter to understand when backend changes
            have been made that might impact determinism.
        object:
          type: string
          description: The object type, which is always "text_completion"
          enum:
            - text_completion
          x-stainless-const: true
        usage:
          $ref: '#/components/schemas/CompletionUsage'
      required:
        - id
        - object
        - created
        - model
        - choices
      x-oaiMeta:
        name: The completion object
        legacy: true
        example: |
          {
            "id": "cmpl-uqkvlQyYK7bGYrRHQ0eXlWi7",
            "object": "text_completion",
            "created": 1589478378,
            "model": "gpt-4-turbo",
            "choices": [
              {
                "text": "\n\nThis is indeed a test",
                "index": 0,
                "logprobs": null,
                "finish_reason": "length"
              }
            ],
            "usage": {
              "prompt_tokens": 5,
              "completion_tokens": 7,
              "total_tokens": 12
            }
          }
    CreateEmbeddingRequest:
      type: object
      additionalProperties: false
      properties:
        input:
          description: >
            Input text to embed, encoded as a string or array of tokens. To embed multiple inputs in a single
            request, pass an array of strings or array of token arrays. The input must not exceed the max
            input tokens for the model (8192 tokens for all embedding models), cannot be an empty string, and
            any array must be 2048 dimensions or less. [Example Python
            code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken) for counting tokens.
            In addition to the per-input token limit, all embedding  models enforce a maximum of 300,000
            tokens summed across all inputs in a  single request.
          example: The quick brown fox jumped over the lazy dog
          oneOf:
            - type: string
              title: string
              description: The string that will be turned into an embedding.
              default: ''
              example: This is a test.
            - type: array
              title: array
              description: The array of strings that will be turned into an embedding.
              minItems: 1
              maxItems: 2048
              items:
                type: string
                default: ''
                example: '[''This is a test.'']'
            - type: array
              title: array
              description: The array of integers that will be turned into an embedding.
              minItems: 1
              maxItems: 2048
              items:
                type: integer
              example: '[1212, 318, 257, 1332, 13]'
            - type: array
              title: array
              description: The array of arrays containing integers that will be turned into an embedding.
              minItems: 1
              maxItems: 2048
              items:
                type: array
                minItems: 1
                items:
                  type: integer
              example: '[[1212, 318, 257, 1332, 13]]'
        model:
          description: >
            ID of the model to use. You can use the [List models](/docs/api-reference/models/list) API to see
            all of your available models, or see our [Model overview](/docs/models) for descriptions of them.
          example: text-embedding-3-small
          anyOf:
            - type: string
            - type: string
              enum:
                - text-embedding-ada-002
                - text-embedding-3-small
                - text-embedding-3-large
          x-oaiTypeLabel: string
        encoding_format:
          description: >-
            The format to return the embeddings in. Can be either `float` or
            [`base64`](https://pypi.org/project/pybase64/).
          example: float
          default: float
          type: string
          enum:
            - float
            - base64
        dimensions:
          description: >
            The number of dimensions the resulting output embeddings should have. Only supported in
            `text-embedding-3` and later models.
          type: integer
          minimum: 1
        user:
          type: string
          example: user-1234
          description: >
            A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse.
            [Learn more](/docs/guides/safety-best-practices#end-user-ids).
      required:
        - model
        - input
    Embedding:
      type: object
      description: |
        Represents an embedding vector returned by embedding endpoint.
      properties:
        index:
          type: integer
          description: The index of the embedding in the list of embeddings.
        embedding:
          type: array
          description: >
            The embedding vector, which is a list of floats. The length of vector depends on the model as
            listed in the [embedding guide](/docs/guides/embeddings).
          items:
            type: number
            format: float
        object:
          type: string
          description: The object type, which is always "embedding".
          enum:
            - embedding
          x-stainless-const: true
      required:
        - index
        - object
        - embedding
      x-oaiMeta:
        name: The embedding object
        example: |
          {
            "object": "embedding",
            "embedding": [
              0.0023064255,
              -0.009327292,
              .... (1536 floats total for ada-002)
              -0.0028842222,
            ],
            "index": 0
          }
    CreateEmbeddingResponse:
      type: object
      properties:
        data:
          type: array
          description: The list of embeddings generated by the model.
          items:
            $ref: '#/components/schemas/Embedding'
        model:
          type: string
          description: The name of the model used to generate the embedding.
        object:
          type: string
          description: The object type, which is always "list".
          enum:
            - list
          x-stainless-const: true
        usage:
          type: object
          description: The usage information for the request.
          properties:
            prompt_tokens:
              type: integer
              description: The number of tokens used by the prompt.
            total_tokens:
              type: integer
              description: The total number of tokens used by the request.
          required:
            - prompt_tokens
            - total_tokens
      required:
        - object
        - model
        - data
        - usage
    Model:
      title: Model
      description: Describes an OpenAI model offering that can be used with the API.
      properties:
        id:
          type: string
          description: The model identifier, which can be referenced in the API endpoints.
        created:
          type: integer
          description: The Unix timestamp (in seconds) when the model was created.
        object:
          type: string
          description: The object type, which is always "model".
          enum:
            - model
          x-stainless-const: true
        owned_by:
          type: string
          description: The organization that owns the model.
      required:
        - id
        - object
        - created
        - owned_by
      x-oaiMeta:
        name: The model object
        example: |
          {
            "id": "VAR_chat_model_id",
            "object": "model",
            "created": 1686935002,
            "owned_by": "openai"
          }
    ListModelsResponse:
      type: object
      properties:
        object:
          type: string
          enum:
            - list
          x-stainless-const: true
        data:
          type: array
          items:
            $ref: '#/components/schemas/Model'
      required:
        - object
        - data
    CreateModerationRequest:
      type: object
      properties:
        input:
          description: |
            Input (or inputs) to classify. Can be a single string, an array of strings, or
            an array of multi-modal input objects similar to other models.
          oneOf:
            - type: string
              description: A string of text to classify for moderation.
              default: ''
              example: I want to kill them.
            - type: array
              description: An array of strings to classify for moderation.
              items:
                type: string
                default: ''
                example: I want to kill them.
            - type: array
              description: An array of multi-modal inputs to the moderation model.
              items:
                oneOf:
                  - type: object
                    description: An object describing an image to classify.
                    properties:
                      type:
                        description: Always `image_url`.
                        type: string
                        enum:
                          - image_url
                        x-stainless-const: true
                      image_url:
                        type: object
                        description: Contains either an image URL or a data URL for a base64 encoded image.
                        properties:
                          url:
                            type: string
                            description: Either a URL of the image or the base64 encoded image data.
                            format: uri
                            example: https://example.com/image.jpg
                        required:
                          - url
                    required:
                      - type
                      - image_url
                  - type: object
                    description: An object describing text to classify.
                    properties:
                      type:
                        description: Always `text`.
                        type: string
                        enum:
                          - text
                        x-stainless-const: true
                      text:
                        description: A string of text to classify.
                        type: string
                        example: I want to kill them
                    required:
                      - type
                      - text
        model:
          description: |
            The content moderation model you would like to use. Learn more in
            [the moderation guide](/docs/guides/moderation), and learn about
            available models [here](/docs/models#moderation).
          nullable: false
          default: omni-moderation-latest
          example: omni-moderation-2024-09-26
          anyOf:
            - type: string
            - type: string
              enum:
                - omni-moderation-latest
                - omni-moderation-2024-09-26
                - text-moderation-latest
                - text-moderation-stable
          x-oaiTypeLabel: string
      required:
        - input
    CreateModerationResponse:
      type: object
      description: Represents if a given text input is potentially harmful.
      properties:
        id:
          type: string
          description: The unique identifier for the moderation request.
        model:
          type: string
          description: The model used to generate the moderation results.
        results:
          type: array
          description: A list of moderation objects.
          items:
            type: object
            properties:
              flagged:
                type: boolean
                description: Whether any of the below categories are flagged.
              categories:
                type: object
                description: A list of the categories, and whether they are flagged or not.
                properties:
                  hate:
                    type: boolean
                    description: >-
                      Content that expresses, incites, or promotes hate based on race, gender, ethnicity,
                      religion, nationality, sexual orientation, disability status, or caste. Hateful content
                      aimed at non-protected groups (e.g., chess players) is harassment.
                  hate/threatening:
                    type: boolean
                    description: >-
                      Hateful content that also includes violence or serious harm towards the targeted group
                      based on race, gender, ethnicity, religion, nationality, sexual orientation, disability
                      status, or caste.
                  harassment:
                    type: boolean
                    description: Content that expresses, incites, or promotes harassing language towards any target.
                  harassment/threatening:
                    type: boolean
                    description: Harassment content that also includes violence or serious harm towards any target.
                  illicit:
                    type: boolean
                    nullable: true
                    description: >-
                      Content that includes instructions or advice that facilitate the planning or execution
                      of wrongdoing, or that gives advice or instruction on how to commit illicit acts. For
                      example, "how to shoplift" would fit this category.
                  illicit/violent:
                    type: boolean
                    nullable: true
                    description: >-
                      Content that includes instructions or advice that facilitate the planning or execution
                      of wrongdoing that also includes violence, or that gives advice or instruction on the
                      procurement of any weapon.
                  self-harm:
                    type: boolean
                    description: >-
                      Content that promotes, encourages, or depicts acts of self-harm, such as suicide,
                      cutting, and eating disorders.
                  self-harm/intent:
                    type: boolean
                    description: >-
                      Content where the speaker expresses that they are engaging or intend to engage in acts
                      of self-harm, such as suicide, cutting, and eating disorders.
                  self-harm/instructions:
                    type: boolean
                    description: >-
                      Content that encourages performing acts of self-harm, such as suicide, cutting, and
                      eating disorders, or that gives instructions or advice on how to commit such acts.
                  sexual:
                    type: boolean
                    description: >-
                      Content meant to arouse sexual excitement, such as the description of sexual activity,
                      or that promotes sexual services (excluding sex education and wellness).
                  sexual/minors:
                    type: boolean
                    description: Sexual content that includes an individual who is under 18 years old.
                  violence:
                    type: boolean
                    description: Content that depicts death, violence, or physical injury.
                  violence/graphic:
                    type: boolean
                    description: Content that depicts death, violence, or physical injury in graphic detail.
                required:
                  - hate
                  - hate/threatening
                  - harassment
                  - harassment/threatening
                  - illicit
                  - illicit/violent
                  - self-harm
                  - self-harm/intent
                  - self-harm/instructions
                  - sexual
                  - sexual/minors
                  - violence
                  - violence/graphic
              category_scores:
                type: object
                description: A list of the categories along with their scores as predicted by model.
                properties:
                  hate:
                    type: number
                    description: The score for the category 'hate'.
                  hate/threatening:
                    type: number
                    description: The score for the category 'hate/threatening'.
                  harassment:
                    type: number
                    description: The score for the category 'harassment'.
                  harassment/threatening:
                    type: number
                    description: The score for the category 'harassment/threatening'.
                  illicit:
                    type: number
                    description: The score for the category 'illicit'.
                  illicit/violent:
                    type: number
                    description: The score for the category 'illicit/violent'.
                  self-harm:
                    type: number
                    description: The score for the category 'self-harm'.
                  self-harm/intent:
                    type: number
                    description: The score for the category 'self-harm/intent'.
                  self-harm/instructions:
                    type: number
                    description: The score for the category 'self-harm/instructions'.
                  sexual:
                    type: number
                    description: The score for the category 'sexual'.
                  sexual/minors:
                    type: number
                    description: The score for the category 'sexual/minors'.
                  violence:
                    type: number
                    description: The score for the category 'violence'.
                  violence/graphic:
                    type: number
                    description: The score for the category 'violence/graphic'.
                required:
                  - hate
                  - hate/threatening
                  - harassment
                  - harassment/threatening
                  - illicit
                  - illicit/violent
                  - self-harm
                  - self-harm/intent
                  - self-harm/instructions
                  - sexual
                  - sexual/minors
                  - violence
                  - violence/graphic
              category_applied_input_types:
                type: object
                description: A list of the categories along with the input type(s) that the score applies to.
                properties:
                  hate:
                    type: array
                    description: The applied input type(s) for the category 'hate'.
                    items:
                      type: string
                      enum:
                        - text
                      x-stainless-const: true
                  hate/threatening:
                    type: array
                    description: The applied input type(s) for the category 'hate/threatening'.
                    items:
                      type: string
                      enum:
                        - text
                      x-stainless-const: true
                  harassment:
                    type: array
                    description: The applied input type(s) for the category 'harassment'.
                    items:
                      type: string
                      enum:
                        - text
                      x-stainless-const: true
                  harassment/threatening:
                    type: array
                    description: The applied input type(s) for the category 'harassment/threatening'.
                    items:
                      type: string
                      enum:
                        - text
                      x-stainless-const: true
                  illicit:
                    type: array
                    description: The applied input type(s) for the category 'illicit'.
                    items:
                      type: string
                      enum:
                        - text
                      x-stainless-const: true
                  illicit/violent:
                    type: array
                    description: The applied input type(s) for the category 'illicit/violent'.
                    items:
                      type: string
                      enum:
                        - text
                      x-stainless-const: true
                  self-harm:
                    type: array
                    description: The applied input type(s) for the category 'self-harm'.
                    items:
                      type: string
                      enum:
                        - text
                        - image
                  self-harm/intent:
                    type: array
                    description: The applied input type(s) for the category 'self-harm/intent'.
                    items:
                      type: string
                      enum:
                        - text
                        - image
                  self-harm/instructions:
                    type: array
                    description: The applied input type(s) for the category 'self-harm/instructions'.
                    items:
                      type: string
                      enum:
                        - text
                        - image
                  sexual:
                    type: array
                    description: The applied input type(s) for the category 'sexual'.
                    items:
                      type: string
                      enum:
                        - text
                        - image
                  sexual/minors:
                    type: array
                    description: The applied input type(s) for the category 'sexual/minors'.
                    items:
                      type: string
                      enum:
                        - text
                      x-stainless-const: true
                  violence:
                    type: array
                    description: The applied input type(s) for the category 'violence'.
                    items:
                      type: string
                      enum:
                        - text
                        - image
                  violence/graphic:
                    type: array
                    description: The applied input type(s) for the category 'violence/graphic'.
                    items:
                      type: string
                      enum:
                        - text
                        - image
                required:
                  - hate
                  - hate/threatening
                  - harassment
                  - harassment/threatening
                  - illicit
                  - illicit/violent
                  - self-harm
                  - self-harm/intent
                  - self-harm/instructions
                  - sexual
                  - sexual/minors
                  - violence
                  - violence/graphic
            required:
              - flagged
              - categories
              - category_scores
              - category_applied_input_types
      required:
        - id
        - model
        - results
      x-oaiMeta:
        name: The moderation object
        example: |
          {
            "id": "modr-0d9740456c391e43c445bf0f010940c7",
            "model": "omni-moderation-latest",
            "results": [
              {
                "flagged": true,
                "categories": {
                  "harassment": true,
                  "harassment/threatening": true,
                  "sexual": false,
                  "hate": false,
                  "hate/threatening": false,
                  "illicit": false,
                  "illicit/violent": false,
                  "self-harm/intent": false,
                  "self-harm/instructions": false,
                  "self-harm": false,
                  "sexual/minors": false,
                  "violence": true,
                  "violence/graphic": true
                },
                "category_scores": {
                  "harassment": 0.8189693396524255,
                  "harassment/threatening": 0.804985420696006,
                  "sexual": 1.573112165348997e-6,
                  "hate": 0.007562942636942845,
                  "hate/threatening": 0.004208854591835476,
                  "illicit": 0.030535955153511665,
                  "illicit/violent": 0.008925306722380033,
                  "self-harm/intent": 0.00023023930975076432,
                  "self-harm/instructions": 0.0002293869201073356,
                  "self-harm": 0.012598046106750154,
                  "sexual/minors": 2.212566909570261e-8,
                  "violence": 0.9999992735124786,
                  "violence/graphic": 0.843064871157054
                },
                "category_applied_input_types": {
                  "harassment": [
                    "text"
                  ],
                  "harassment/threatening": [
                    "text"
                  ],
                  "sexual": [
                    "text",
                    "image"
                  ],
                  "hate": [
                    "text"
                  ],
                  "hate/threatening": [
                    "text"
                  ],
                  "illicit": [
                    "text"
                  ],
                  "illicit/violent": [
                    "text"
                  ],
                  "self-harm/intent": [
                    "text",
                    "image"
                  ],
                  "self-harm/instructions": [
                    "text",
                    "image"
                  ],
                  "self-harm": [
                    "text",
                    "image"
                  ],
                  "sexual/minors": [
                    "text"
                  ],
                  "violence": [
                    "text",
                    "image"
                  ],
                  "violence/graphic": [
                    "text",
                    "image"
                  ]
                }
              }
            ]
          }
  securitySchemes:
    ApiKeyAuth:
      type: http
      scheme: bearer
      bearerFormat: JWT